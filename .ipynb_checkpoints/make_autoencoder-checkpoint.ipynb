{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import os                                                                          \n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"                                       \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"                                             \n",
    "import tensorflow as tf                                                            \n",
    "session_config=tf.ConfigProto(                                                     \n",
    "        gpu_options=tf.GPUOptions(per_process_gpu_memory_fraction=0.9, allow_growth=True), allow_soft_placement=True, log_device_placement=False)\n",
    "sess = tf.Session(config=session_config)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import pickle\n",
    "from skimage import io, transform\n",
    "import os\n",
    "import numpy as np\n",
    "from skimage import io\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import keras.backend as K\n",
    "from keras import layers, Model\n",
    "from keras.optimizers import Adam, SGD\n",
    "from keras.applications.resnet50 import ResNet50, preprocess_input\n",
    "from keras.layers import Conv2D, BatchNormalization, Activation, ZeroPadding2D, \\\n",
    "Conv2DTranspose, UpSampling2D, Input, GlobalMaxPool2D, LeakyReLU\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# address = '/all/media/hdd1/georgy'\n",
    "# def fix_address(adr):\n",
    "#     fix_adr = list(adr)\n",
    "#     fix_adr[0]=address\n",
    "#     fix_adr=''.join(fix_adr)\n",
    "#     return fix_adr\n",
    "# with open('/all/media/hdd1/georgy/checkpoints/CubeChangeClassName_v1.2_558b0595e105de8cad848fa41ae1bc87de6713ec62906783b57858e9.pkl','rb') as f:\n",
    "#     data = pickle.load(f)\n",
    "\n",
    "# patch_dir_name = 'patches'\n",
    "# if not os.path.isdir(patch_dir_name):\n",
    "#     os.makedirs(patch_dir_name)\n",
    "\n",
    "# sizes = []\n",
    "# for i, example in enumerate(data['data_description']):\n",
    "#     image = io.imread(fix_address(example['image_path']))\n",
    "#     for j, obj in enumerate(example['objects']):\n",
    "#         x1,y1 = np.min(obj['polygons']['front'], axis=0).astype(int)\n",
    "#         x2,y2 = np.max(obj['polygons']['front'],axis=0).astype(int)\n",
    "#         patch = image[y1:y2,x1:x2].copy()\n",
    "#         io.imsave(patch_dir_name + '/{}_{}.jpg'.format(i,j), patch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## define loss pad and crop func "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# smooth=1\n",
    "# def dice_coef(y_true, y_pred):\n",
    "#     y_true_f = K.flatten(y_true)\n",
    "#     y_pred_f = K.flatten(y_pred)\n",
    "#     intersection = K.sum(y_true_f * y_pred_f)\n",
    "#     return (2. * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n",
    "\n",
    "# def dice_coef_loss(y_true, y_pred):\n",
    "#     return -dice_coef(y_true, y_pred)\n",
    "\n",
    "def padd_to_16fit(inputs):\n",
    "    '''\n",
    "    zero pad input tensor to shape multiple to 16\n",
    "    '''\n",
    "    shape = K.shape(inputs) #batch,h,w,channel\n",
    "    inputs = K.switch(K.equal(shape[2]%32,0),\n",
    "                            inputs,\n",
    "                            tf.pad(inputs,((0,0), #batch\n",
    "                                           (0,0), #h\n",
    "                                           ((32-shape[2]%32)//2,(32-shape[2]%32)//2+(32-shape[2]%32)%2), #w\n",
    "                                           (0,0)))) #channel\n",
    "    inputs = K.switch(K.equal(shape[1]%32,0),\n",
    "                            inputs,\n",
    "                            tf.pad(inputs,((0,0),\n",
    "                                          ((32-shape[1]%32)//2,(32-shape[1]%32)//2+(32-shape[1]%32)%2),\n",
    "                                          (0,0),\n",
    "                                          (0,0))))\n",
    "    return inputs\n",
    "\n",
    "def crop_to_16fit(arg):\n",
    "    '''\n",
    "    crop output tensor to initial shape (undo result of padd_to_16fit() )\n",
    "    '''\n",
    "    [conv10, inputs] = arg\n",
    "    shape = K.shape(inputs) #batch,h,w,channel\n",
    "    conv10 = tf.cond(K.equal(shape[1]%32,0),\n",
    "                            lambda: conv10,\n",
    "                            lambda: tf.slice(conv10,(0,(32-shape[1]%32)//2,0,0),\n",
    "                                           (-1,shape[1],-1,-1)),\n",
    "                    name = 'vertical_crop') \n",
    "    conv10 = tf.cond(K.equal(shape[2]%32,0),\n",
    "                            lambda: conv10,\n",
    "                            lambda: tf.slice(conv10,(0,0,(32-shape[2]%32)//2,0),\n",
    "                                           (-1,-1,shape[2],-1)),\n",
    "                    name = 'horizontal_crop')\n",
    "    return conv10\n",
    "\n",
    "def make_voluem(inputs):\n",
    "    x_flatten, x_shaped = inputs\n",
    "    shape = tf.shape(x_shaped)\n",
    "    ones_voluem = tf.ones([shape[0], shape[1], shape[2], HIDDEN_DEPTH], tf.float32)\n",
    "    return x_flatten*ones_voluem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## define  upsample layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identity_block(input_tensor, kernel_size, filters, stage, block):\n",
    "    \"\"\"The identity block is the block that has no conv layer at shortcut.\n",
    "    # Arguments\n",
    "        input_tensor: input tensor\n",
    "        kernel_size: default 3, the kernel size of middle conv layer at main path\n",
    "        filters: list of integers, the filters of 3 conv layer at main path\n",
    "        stage: integer, current stage label, used for generating layer names\n",
    "        block: 'a','b'..., current block label, used for generating layer names\n",
    "    # Returns\n",
    "        Output tensor for the block.\n",
    "    \"\"\"\n",
    "    filters1, filters2, filters3 = filters\n",
    "    if K.image_data_format() == 'channels_last':\n",
    "        bn_axis = -1\n",
    "    else:\n",
    "        bn_axis = 1\n",
    "    conv_name_base = 'upsample_res' + str(stage) + block + '_branch'\n",
    "    bn_name_base = 'upsample_bn' + str(stage) + block + '_branch'\n",
    "\n",
    "    x = Conv2D(filters1, (1, 1), name=conv_name_base + '2a')(input_tensor)\n",
    "    x = BatchNormalization(axis=bn_axis, name=bn_name_base + '2a')(x)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    x = Conv2D(filters2, kernel_size,\n",
    "               padding='same', name=conv_name_base + '2b')(x)\n",
    "    x = BatchNormalization(axis=bn_axis, name=bn_name_base + '2b')(x)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    x = Conv2D(filters3, (1, 1), name=conv_name_base + '2c')(x)\n",
    "    x = BatchNormalization(axis=bn_axis, name=bn_name_base + '2c')(x)\n",
    "\n",
    "    x = layers.add([input_tensor, x])\n",
    "    x = Activation('relu')(x)\n",
    "    return x\n",
    "\n",
    "def conv_block(input_tensor, kernel_size, filters, stage, block, strides=(2, 2)):\n",
    "    \"\"\"A block that has a conv layer at shortcut.\n",
    "    # Arguments\n",
    "        input_tensor: input tensor\n",
    "        kernel_size: default 3, the kernel size of middle conv layer at main path\n",
    "        filters: list of integers, the filters of 3 conv layer at main path\n",
    "        stage: integer, current stage label, used for generating layer names\n",
    "        block: 'a','b'..., current block label, used for generating layer names\n",
    "        strides: Strides for the first conv layer in the block.\n",
    "    # Returns\n",
    "        Output tensor for the block.\n",
    "    Note that from stage 3,\n",
    "    the first conv layer at main path is with strides=(2, 2)\n",
    "    And the shortcut should have strides=(2, 2) as well\n",
    "    \"\"\"\n",
    "    filters1, filters2, filters3 = filters\n",
    "    if K.image_data_format() == 'channels_last':\n",
    "        bn_axis = -1\n",
    "    else:\n",
    "        bn_axis = 1\n",
    "    conv_name_base = 'upsample_res' + str(stage) + block + '_branch'\n",
    "    bn_name_base = 'upsample_bn' + str(stage) + block + '_branch'\n",
    "    up_name_base = 'upsample_upsample' + str(stage) + block + '_branch'\n",
    "    \n",
    "    x = UpSampling2D(strides, name=up_name_base + '2a')(input_tensor)\n",
    "    x = Conv2D(filters1, (1, 1), strides=(1, 1),\n",
    "               name=conv_name_base + '2a')(x)\n",
    "    x = BatchNormalization(axis=bn_axis, name=bn_name_base + '2a')(x)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    x = Conv2D(filters2, kernel_size, padding='same',\n",
    "               name=conv_name_base + '2b')(x)\n",
    "    x = BatchNormalization(axis=bn_axis, name=bn_name_base + '2b')(x)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    x = Conv2D(filters3, (1, 1), name=conv_name_base + '2c')(x)\n",
    "    x = BatchNormalization(axis=bn_axis, name=bn_name_base + '2c')(x)\n",
    "        \n",
    "    shortcut = UpSampling2D(strides, name=up_name_base + '1')(input_tensor)\n",
    "    shortcut = Conv2D(filters3, (1, 1), strides=(1, 1),\n",
    "                      name=conv_name_base + '1')(shortcut)\n",
    "    shortcut = BatchNormalization(axis=bn_axis, name=bn_name_base + '1')(shortcut)\n",
    "\n",
    "    x = layers.add([x, shortcut])\n",
    "    x = Activation('relu')(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  set nontrainable layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_trainable(model):\n",
    "    for layer in model.layers:\n",
    "        if not layer.name.startswith('upsample'):\n",
    "            layer.trainable = False\n",
    "        if 'bn' in layer.name:\n",
    "            layer.trainable = True\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "HIDDEN_DEPTH = 2048\n",
    "\n",
    "inputs = keras.layers.Input((None, None, 3))\n",
    "x = keras.layers.Lambda(padd_to_16fit, name='pad', output_shape=(None,None,3))(inputs)\n",
    "resnet_backbone = ResNet50(include_top=False, weights='imagenet', input_tensor=x, input_shape=None, pooling='max')\n",
    "x_shaped = resnet_backbone.get_layer(index=-3).output\n",
    "x = GlobalMaxPool2D()(x_shaped)\n",
    "x = keras.layers.Dense(HIDDEN_DEPTH, name = 'upsample_dense', activation='relu')(x)\n",
    "\n",
    "x = keras.layers.Lambda(make_voluem)([x, x_shaped])\n",
    "\n",
    "bn_axis = -1 # tf order\n",
    "x = conv_block(x, 3, [2048, 512, 512], stage=5, block='a')\n",
    "x = identity_block(x, 3, [2048, 512, 512], stage=5, block='b')\n",
    "x = identity_block(x, 3, [2048, 512, 512], stage=5, block='c')\n",
    "\n",
    "x = conv_block(x, 3, [1024, 256, 256], stage=4, block='a')\n",
    "x = identity_block(x, 3, [1024, 256, 256], stage=4, block='b')\n",
    "x = identity_block(x, 3, [1024, 256, 256], stage=4, block='c')\n",
    "x = identity_block(x, 3, [1024, 256, 256], stage=4, block='d')\n",
    "x = identity_block(x, 3, [1024, 256, 256], stage=4, block='e')\n",
    "x = identity_block(x, 3, [1024, 256, 256], stage=4, block='f')    \n",
    "\n",
    "x = conv_block(x, 3, [512, 128, 128], stage=3, block='a')\n",
    "x = identity_block(x, 3, [512, 128, 128], stage=3, block='b')\n",
    "x = identity_block(x, 3, [512, 128, 128], stage=3, block='c')\n",
    "x = identity_block(x, 3, [512, 128, 128], stage=3, block='d')\n",
    "\n",
    "x = conv_block(x, 3, [256, 64, 64], stage=2, block='a', strides=(2, 2))\n",
    "x = identity_block(x, 3, [256, 64, 64], stage=2, block='b')\n",
    "x = identity_block(x, 3, [256, 64, 64], stage=2, block='c')\n",
    "\n",
    "x = UpSampling2D((2,2), name = 'upsample_last_upsample')(x)\n",
    "x = Conv2D(3, (3, 3), padding='same', name='upsample_conv1')(x)\n",
    "# x = BatchNormalization(axis=bn_axis, name='upsample_bn_conv1')(x)\n",
    "x = Activation('linear')(x)\n",
    "\n",
    "x = keras.layers.Lambda(crop_to_16fit, name='crop')([x, inputs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(inputs=inputs, outputs=x)\n",
    "model = set_trainable(model) # not train initial resnet\n",
    "model.compile(optimizer=SGD(lr=1e-3), loss='mse')\n",
    "model.load_weights('/root/.keras/models/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5', by_name=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.trainable_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = io.imread('../patches/1001_0.jpg')\n",
    "# input_image /= 255\n",
    "input_image = image.copy().astype(np.float)\n",
    "input_image /= 127.5\n",
    "input_image -= 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# small_image = transform.rescale(input_image, scale = 0.1, order = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "plateau = keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=10, verbose=1, mode='auto', epsilon=0.0001, cooldown=0, min_lr=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1 samples, validate on 1 samples\n",
      "Epoch 1/5000\n",
      "1/1 [==============================] - 0s 306ms/step - loss: 0.1863 - val_loss: 211372496.0000\n",
      "Epoch 2/5000\n",
      "1/1 [==============================] - 0s 265ms/step - loss: 0.1868 - val_loss: 202506896.0000\n",
      "Epoch 3/5000\n",
      "1/1 [==============================] - 0s 260ms/step - loss: 0.1873 - val_loss: 200960112.0000\n",
      "Epoch 4/5000\n",
      "1/1 [==============================] - 0s 258ms/step - loss: 0.1871 - val_loss: 180811088.0000\n",
      "Epoch 5/5000\n",
      "1/1 [==============================] - 0s 259ms/step - loss: 0.1871 - val_loss: 185516544.0000\n",
      "Epoch 6/5000\n",
      "1/1 [==============================] - 0s 258ms/step - loss: 0.1871 - val_loss: 168693632.0000\n",
      "Epoch 7/5000\n",
      "1/1 [==============================] - 0s 260ms/step - loss: 0.1880 - val_loss: 169584048.0000\n",
      "Epoch 8/5000\n",
      "1/1 [==============================] - 0s 260ms/step - loss: 0.1878 - val_loss: 156952368.0000\n",
      "Epoch 9/5000\n",
      "1/1 [==============================] - 0s 259ms/step - loss: 0.1878 - val_loss: 151869104.0000\n",
      "Epoch 10/5000\n",
      "1/1 [==============================] - 0s 259ms/step - loss: 0.1869 - val_loss: 149285584.0000\n",
      "Epoch 11/5000\n",
      "1/1 [==============================] - 0s 259ms/step - loss: 0.1864 - val_loss: 141712192.0000\n",
      "Epoch 12/5000\n",
      "1/1 [==============================] - 0s 259ms/step - loss: 0.1862 - val_loss: 138888000.0000\n",
      "Epoch 13/5000\n",
      "1/1 [==============================] - 0s 259ms/step - loss: 0.1860 - val_loss: 134266752.0000\n",
      "Epoch 14/5000\n",
      "1/1 [==============================] - 0s 259ms/step - loss: 0.1860 - val_loss: 130203128.0000\n",
      "Epoch 15/5000\n",
      "1/1 [==============================] - 0s 260ms/step - loss: 0.1854 - val_loss: 126198040.0000\n",
      "Epoch 16/5000\n",
      "1/1 [==============================] - 0s 260ms/step - loss: 0.1850 - val_loss: 121002720.0000\n",
      "Epoch 17/5000\n",
      "1/1 [==============================] - 0s 260ms/step - loss: 0.1849 - val_loss: 117276424.0000\n",
      "Epoch 18/5000\n",
      "1/1 [==============================] - 0s 258ms/step - loss: 0.1849 - val_loss: 111719824.0000\n",
      "Epoch 19/5000\n",
      "1/1 [==============================] - 0s 260ms/step - loss: 0.1851 - val_loss: 110313664.0000\n",
      "Epoch 20/5000\n",
      "1/1 [==============================] - 0s 258ms/step - loss: 0.1854 - val_loss: 102956352.0000\n",
      "Epoch 21/5000\n",
      "1/1 [==============================] - 0s 259ms/step - loss: 0.1857 - val_loss: 103845728.0000\n",
      "Epoch 22/5000\n",
      "1/1 [==============================] - 0s 259ms/step - loss: 0.1854 - val_loss: 97212152.0000\n",
      "Epoch 23/5000\n",
      "1/1 [==============================] - 0s 259ms/step - loss: 0.1853 - val_loss: 96518856.0000\n",
      "Epoch 24/5000\n",
      "1/1 [==============================] - 0s 258ms/step - loss: 0.1849 - val_loss: 91761624.0000\n",
      "Epoch 25/5000\n",
      "1/1 [==============================] - 0s 260ms/step - loss: 0.1847 - val_loss: 89731736.0000\n",
      "Epoch 26/5000\n",
      "1/1 [==============================] - 0s 259ms/step - loss: 0.1848 - val_loss: 86134760.0000\n",
      "Epoch 27/5000\n",
      "1/1 [==============================] - 0s 258ms/step - loss: 0.1847 - val_loss: 84847744.0000\n",
      "Epoch 28/5000\n",
      "1/1 [==============================] - 0s 259ms/step - loss: 0.1847 - val_loss: 80518016.0000\n",
      "Epoch 29/5000\n",
      "1/1 [==============================] - 0s 257ms/step - loss: 0.1846 - val_loss: 80258928.0000\n",
      "Epoch 30/5000\n",
      "1/1 [==============================] - 0s 260ms/step - loss: 0.1842 - val_loss: 75672224.0000\n",
      "Epoch 31/5000\n",
      "1/1 [==============================] - 0s 259ms/step - loss: 0.1837 - val_loss: 75312872.0000\n",
      "Epoch 32/5000\n",
      "1/1 [==============================] - 0s 259ms/step - loss: 0.1838 - val_loss: 71388560.0000\n",
      "Epoch 33/5000\n",
      "1/1 [==============================] - 0s 260ms/step - loss: 0.1838 - val_loss: 70120496.0000\n",
      "Epoch 34/5000\n",
      "1/1 [==============================] - 0s 259ms/step - loss: 0.1839 - val_loss: 67876296.0000\n",
      "Epoch 35/5000\n",
      "1/1 [==============================] - 0s 259ms/step - loss: 0.1840 - val_loss: 65944792.0000\n",
      "Epoch 36/5000\n",
      "1/1 [==============================] - 0s 258ms/step - loss: 0.1849 - val_loss: 63636868.0000\n",
      "Epoch 37/5000\n",
      "1/1 [==============================] - 0s 261ms/step - loss: 0.1855 - val_loss: 62586668.0000\n",
      "Epoch 38/5000\n",
      "1/1 [==============================] - 0s 259ms/step - loss: 0.1869 - val_loss: 59104564.0000\n",
      "Epoch 39/5000\n",
      "1/1 [==============================] - 0s 259ms/step - loss: 0.1868 - val_loss: 59682228.0000\n",
      "Epoch 40/5000\n",
      "1/1 [==============================] - 0s 259ms/step - loss: 0.1859 - val_loss: 56386708.0000\n",
      "Epoch 41/5000\n",
      "1/1 [==============================] - 0s 259ms/step - loss: 0.1853 - val_loss: 55053840.0000\n",
      "Epoch 42/5000\n",
      "1/1 [==============================] - 0s 259ms/step - loss: 0.1848 - val_loss: 52899736.0000\n",
      "Epoch 43/5000\n",
      "1/1 [==============================] - 0s 258ms/step - loss: 0.1847 - val_loss: 53861344.0000\n",
      "Epoch 44/5000\n",
      "1/1 [==============================] - 0s 260ms/step - loss: 0.1851 - val_loss: 49253976.0000\n",
      "Epoch 45/5000\n",
      "1/1 [==============================] - 0s 259ms/step - loss: 0.1851 - val_loss: 49983764.0000\n",
      "Epoch 46/5000\n",
      "1/1 [==============================] - 0s 259ms/step - loss: 0.1849 - val_loss: 46407436.0000\n",
      "Epoch 47/5000\n",
      "1/1 [==============================] - 0s 260ms/step - loss: 0.1847 - val_loss: 46963516.0000\n",
      "Epoch 48/5000\n",
      "1/1 [==============================] - 0s 260ms/step - loss: 0.1847 - val_loss: 44421640.0000\n",
      "Epoch 49/5000\n",
      "1/1 [==============================] - 0s 259ms/step - loss: 0.1846 - val_loss: 44870984.0000\n",
      "Epoch 50/5000\n",
      "1/1 [==============================] - 0s 261ms/step - loss: 0.1840 - val_loss: 42121648.0000\n",
      "Epoch 51/5000\n",
      "1/1 [==============================] - 0s 260ms/step - loss: 0.1836 - val_loss: 42718260.0000\n",
      "Epoch 52/5000\n",
      "1/1 [==============================] - 0s 260ms/step - loss: 0.1834 - val_loss: 39483472.0000\n",
      "Epoch 53/5000\n",
      "1/1 [==============================] - 0s 260ms/step - loss: 0.1832 - val_loss: 40144460.0000\n",
      "Epoch 54/5000\n",
      "1/1 [==============================] - 0s 258ms/step - loss: 0.1830 - val_loss: 38008664.0000\n",
      "Epoch 55/5000\n",
      "1/1 [==============================] - 0s 259ms/step - loss: 0.1830 - val_loss: 38214484.0000\n",
      "Epoch 56/5000\n",
      "1/1 [==============================] - 0s 260ms/step - loss: 0.1831 - val_loss: 36176472.0000\n",
      "Epoch 57/5000\n",
      "1/1 [==============================] - 0s 260ms/step - loss: 0.1831 - val_loss: 36090748.0000\n",
      "Epoch 58/5000\n",
      "1/1 [==============================] - 0s 259ms/step - loss: 0.1832 - val_loss: 34285196.0000\n",
      "Epoch 59/5000\n",
      "1/1 [==============================] - 0s 262ms/step - loss: 0.1835 - val_loss: 34090572.0000\n",
      "Epoch 60/5000\n",
      "1/1 [==============================] - 0s 260ms/step - loss: 0.1845 - val_loss: 32050228.0000\n",
      "Epoch 61/5000\n",
      "1/1 [==============================] - 0s 260ms/step - loss: 0.1846 - val_loss: 31851964.0000\n",
      "Epoch 62/5000\n",
      "1/1 [==============================] - 0s 259ms/step - loss: 0.1836 - val_loss: 31585652.0000\n",
      "Epoch 63/5000\n",
      "1/1 [==============================] - 0s 259ms/step - loss: 0.1830 - val_loss: 30125876.0000\n",
      "Epoch 64/5000\n",
      "1/1 [==============================] - 0s 258ms/step - loss: 0.1826 - val_loss: 30173374.0000\n",
      "Epoch 65/5000\n",
      "1/1 [==============================] - 0s 260ms/step - loss: 0.1824 - val_loss: 28862258.0000\n",
      "Epoch 66/5000\n",
      "1/1 [==============================] - 0s 259ms/step - loss: 0.1822 - val_loss: 28511360.0000\n",
      "Epoch 67/5000\n",
      "1/1 [==============================] - 0s 259ms/step - loss: 0.1821 - val_loss: 27271876.0000\n",
      "Epoch 68/5000\n",
      "1/1 [==============================] - 0s 260ms/step - loss: 0.1823 - val_loss: 27145480.0000\n",
      "Epoch 69/5000\n",
      "1/1 [==============================] - 0s 264ms/step - loss: 0.1824 - val_loss: 26018288.0000\n",
      "Epoch 70/5000\n",
      "1/1 [==============================] - 0s 261ms/step - loss: 0.1829 - val_loss: 25977364.0000\n",
      "Epoch 71/5000\n",
      "1/1 [==============================] - 0s 261ms/step - loss: 0.1828 - val_loss: 24920140.0000\n",
      "Epoch 72/5000\n",
      "1/1 [==============================] - 0s 261ms/step - loss: 0.1830 - val_loss: 24798796.0000\n",
      "Epoch 73/5000\n",
      "1/1 [==============================] - 0s 259ms/step - loss: 0.1825 - val_loss: 24217046.0000\n",
      "Epoch 74/5000\n",
      "1/1 [==============================] - 0s 260ms/step - loss: 0.1822 - val_loss: 23458854.0000\n",
      "Epoch 75/5000\n",
      "1/1 [==============================] - 0s 259ms/step - loss: 0.1820 - val_loss: 23051232.0000\n",
      "Epoch 76/5000\n",
      "1/1 [==============================] - 0s 260ms/step - loss: 0.1819 - val_loss: 22257324.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 77/5000\n",
      "1/1 [==============================] - 0s 258ms/step - loss: 0.1822 - val_loss: 22044936.0000\n",
      "Epoch 78/5000\n",
      "1/1 [==============================] - 0s 259ms/step - loss: 0.1828 - val_loss: 20745132.0000\n",
      "Epoch 79/5000\n",
      "1/1 [==============================] - 0s 260ms/step - loss: 0.1836 - val_loss: 21217140.0000\n",
      "Epoch 80/5000\n",
      "1/1 [==============================] - 0s 259ms/step - loss: 0.1840 - val_loss: 19684908.0000\n",
      "Epoch 81/5000\n",
      "1/1 [==============================] - 0s 259ms/step - loss: 0.1851 - val_loss: 20121818.0000\n",
      "Epoch 82/5000\n",
      "1/1 [==============================] - 0s 259ms/step - loss: 0.1860 - val_loss: 19060966.0000\n",
      "Epoch 83/5000\n",
      "1/1 [==============================] - 0s 260ms/step - loss: 0.1853 - val_loss: 19323704.0000\n",
      "Epoch 84/5000\n",
      "1/1 [==============================] - 0s 260ms/step - loss: 0.1841 - val_loss: 18325114.0000\n",
      "Epoch 85/5000\n",
      "1/1 [==============================] - 0s 258ms/step - loss: 0.1840 - val_loss: 18115018.0000\n",
      "Epoch 86/5000\n",
      "1/1 [==============================] - 0s 260ms/step - loss: 0.1836 - val_loss: 17447662.0000\n",
      "Epoch 87/5000\n",
      "1/1 [==============================] - 0s 259ms/step - loss: 0.1835 - val_loss: 17236322.0000\n",
      "Epoch 88/5000\n",
      "1/1 [==============================] - 0s 260ms/step - loss: 0.1832 - val_loss: 16753829.0000\n",
      "Epoch 89/5000\n",
      "1/1 [==============================] - 0s 259ms/step - loss: 0.1832 - val_loss: 16387463.0000\n",
      "Epoch 90/5000\n",
      "1/1 [==============================] - 0s 258ms/step - loss: 0.1828 - val_loss: 16046229.0000\n",
      "Epoch 91/5000\n",
      "1/1 [==============================] - 0s 259ms/step - loss: 0.1827 - val_loss: 15567107.0000\n",
      "Epoch 92/5000\n",
      "1/1 [==============================] - 0s 258ms/step - loss: 0.1824 - val_loss: 15270688.0000\n",
      "Epoch 93/5000\n",
      "1/1 [==============================] - 0s 260ms/step - loss: 0.1826 - val_loss: 14906099.0000\n",
      "Epoch 94/5000\n",
      "1/1 [==============================] - 0s 258ms/step - loss: 0.1827 - val_loss: 14315699.0000\n",
      "Epoch 95/5000\n",
      "1/1 [==============================] - 0s 259ms/step - loss: 0.1824 - val_loss: 14083792.0000\n",
      "Epoch 96/5000\n",
      "1/1 [==============================] - 0s 261ms/step - loss: 0.1822 - val_loss: 13570704.0000\n",
      "Epoch 97/5000\n",
      "1/1 [==============================] - 0s 260ms/step - loss: 0.1821 - val_loss: 13289378.0000\n",
      "Epoch 98/5000\n",
      "1/1 [==============================] - 0s 260ms/step - loss: 0.1822 - val_loss: 13024854.0000\n",
      "Epoch 99/5000\n",
      "1/1 [==============================] - 0s 259ms/step - loss: 0.1823 - val_loss: 12456763.0000\n",
      "Epoch 100/5000\n",
      "1/1 [==============================] - 0s 260ms/step - loss: 0.1824 - val_loss: 12350292.0000\n",
      "Epoch 101/5000\n",
      "1/1 [==============================] - 0s 259ms/step - loss: 0.1834 - val_loss: 11899892.0000\n",
      "Epoch 102/5000\n",
      "1/1 [==============================] - 0s 259ms/step - loss: 0.1846 - val_loss: 11912239.0000\n",
      "Epoch 103/5000\n",
      "1/1 [==============================] - 0s 260ms/step - loss: 0.1871 - val_loss: 11356129.0000\n",
      "Epoch 104/5000\n",
      "1/1 [==============================] - 0s 261ms/step - loss: 0.1857 - val_loss: 11185158.0000\n",
      "Epoch 105/5000\n",
      "1/1 [==============================] - 0s 259ms/step - loss: 0.1839 - val_loss: 10915747.0000\n",
      "Epoch 106/5000\n",
      "1/1 [==============================] - 0s 262ms/step - loss: 0.1835 - val_loss: 10632276.0000\n",
      "Epoch 107/5000\n",
      "1/1 [==============================] - 0s 261ms/step - loss: 0.1828 - val_loss: 10292757.0000\n",
      "Epoch 108/5000\n",
      "1/1 [==============================] - 0s 259ms/step - loss: 0.1823 - val_loss: 10122465.0000\n",
      "Epoch 109/5000\n",
      "1/1 [==============================] - 0s 261ms/step - loss: 0.1816 - val_loss: 9741738.0000\n",
      "Epoch 110/5000\n",
      "1/1 [==============================] - 0s 259ms/step - loss: 0.1815 - val_loss: 9605027.0000\n",
      "Epoch 111/5000\n",
      "1/1 [==============================] - 0s 262ms/step - loss: 0.1816 - val_loss: 9308665.0000\n",
      "Epoch 112/5000\n",
      "1/1 [==============================] - 0s 260ms/step - loss: 0.1820 - val_loss: 9166981.0000\n",
      "Epoch 113/5000\n",
      "1/1 [==============================] - 0s 261ms/step - loss: 0.1818 - val_loss: 8833124.0000\n",
      "Epoch 114/5000\n",
      "1/1 [==============================] - 0s 261ms/step - loss: 0.1821 - val_loss: 8661211.0000\n",
      "Epoch 115/5000\n",
      "1/1 [==============================] - 0s 259ms/step - loss: 0.1817 - val_loss: 8293837.0000\n",
      "Epoch 116/5000\n",
      "1/1 [==============================] - 0s 259ms/step - loss: 0.1815 - val_loss: 8251705.0000\n",
      "Epoch 117/5000\n",
      "1/1 [==============================] - 0s 260ms/step - loss: 0.1815 - val_loss: 7985334.0000\n",
      "Epoch 118/5000\n",
      "1/1 [==============================] - 0s 260ms/step - loss: 0.1812 - val_loss: 7757914.5000\n",
      "Epoch 119/5000\n",
      "1/1 [==============================] - 0s 264ms/step - loss: 0.1815 - val_loss: 7645805.0000\n",
      "Epoch 120/5000\n",
      "1/1 [==============================] - 0s 259ms/step - loss: 0.1814 - val_loss: 7412634.5000\n",
      "Epoch 121/5000\n",
      "1/1 [==============================] - 0s 263ms/step - loss: 0.1821 - val_loss: 7163736.5000\n",
      "Epoch 122/5000\n",
      "1/1 [==============================] - 0s 273ms/step - loss: 0.1816 - val_loss: 7163646.0000\n",
      "Epoch 123/5000\n",
      "1/1 [==============================] - 0s 273ms/step - loss: 0.1813 - val_loss: 6831867.0000\n",
      "Epoch 124/5000\n",
      "1/1 [==============================] - 0s 272ms/step - loss: 0.1809 - val_loss: 6703467.0000\n",
      "Epoch 125/5000\n",
      "1/1 [==============================] - 0s 270ms/step - loss: 0.1811 - val_loss: 6493734.5000\n",
      "Epoch 126/5000\n",
      "1/1 [==============================] - 0s 266ms/step - loss: 0.1808 - val_loss: 6384221.0000\n",
      "Epoch 127/5000\n",
      "1/1 [==============================] - 0s 265ms/step - loss: 0.1809 - val_loss: 6262143.0000\n",
      "Epoch 128/5000\n",
      "1/1 [==============================] - 0s 261ms/step - loss: 0.1810 - val_loss: 5990444.0000\n",
      "Epoch 129/5000\n",
      "1/1 [==============================] - 0s 261ms/step - loss: 0.1816 - val_loss: 5988450.5000\n",
      "Epoch 130/5000\n",
      "1/1 [==============================] - 0s 258ms/step - loss: 0.1815 - val_loss: 5791708.5000\n",
      "Epoch 131/5000\n",
      "1/1 [==============================] - 0s 259ms/step - loss: 0.1819 - val_loss: 5586713.0000\n",
      "Epoch 132/5000\n",
      "1/1 [==============================] - 0s 260ms/step - loss: 0.1814 - val_loss: 5490892.5000\n",
      "Epoch 133/5000\n",
      "1/1 [==============================] - 0s 261ms/step - loss: 0.1811 - val_loss: 5481704.0000\n",
      "Epoch 134/5000\n",
      "1/1 [==============================] - 0s 260ms/step - loss: 0.1806 - val_loss: 5280617.0000\n",
      "Epoch 135/5000\n",
      "1/1 [==============================] - 0s 261ms/step - loss: 0.1805 - val_loss: 5207805.5000\n",
      "Epoch 136/5000\n",
      "1/1 [==============================] - 0s 260ms/step - loss: 0.1805 - val_loss: 4971401.0000\n",
      "Epoch 137/5000\n",
      "1/1 [==============================] - 0s 258ms/step - loss: 0.1804 - val_loss: 4929067.5000\n",
      "Epoch 138/5000\n",
      "1/1 [==============================] - 0s 260ms/step - loss: 0.1808 - val_loss: 4664997.5000\n",
      "Epoch 139/5000\n",
      "1/1 [==============================] - 0s 259ms/step - loss: 0.1820 - val_loss: 4633439.5000\n",
      "Epoch 140/5000\n",
      "1/1 [==============================] - 0s 260ms/step - loss: 0.1828 - val_loss: 4516557.0000\n",
      "Epoch 141/5000\n",
      "1/1 [==============================] - 0s 259ms/step - loss: 0.1827 - val_loss: 4359963.5000\n",
      "Epoch 142/5000\n",
      "1/1 [==============================] - 0s 259ms/step - loss: 0.1823 - val_loss: 4259353.0000\n",
      "Epoch 143/5000\n",
      "1/1 [==============================] - 0s 262ms/step - loss: 0.1816 - val_loss: 4165450.5000\n",
      "Epoch 144/5000\n",
      "1/1 [==============================] - 0s 270ms/step - loss: 0.1809 - val_loss: 4022240.5000\n",
      "Epoch 145/5000\n",
      "1/1 [==============================] - 0s 288ms/step - loss: 0.1805 - val_loss: 3946455.7500\n",
      "Epoch 146/5000\n",
      "1/1 [==============================] - 0s 289ms/step - loss: 0.1808 - val_loss: 3845253.5000\n",
      "Epoch 147/5000\n",
      "1/1 [==============================] - 0s 291ms/step - loss: 0.1810 - val_loss: 3731153.5000\n",
      "Epoch 148/5000\n",
      "1/1 [==============================] - 0s 274ms/step - loss: 0.1809 - val_loss: 3621419.5000\n",
      "Epoch 149/5000\n",
      "1/1 [==============================] - 0s 273ms/step - loss: 0.1809 - val_loss: 3502541.5000\n",
      "Epoch 150/5000\n",
      "1/1 [==============================] - 0s 269ms/step - loss: 0.1809 - val_loss: 3428754.0000\n",
      "Epoch 151/5000\n",
      "1/1 [==============================] - 0s 266ms/step - loss: 0.1809 - val_loss: 3371501.7500\n",
      "Epoch 152/5000\n",
      "1/1 [==============================] - 0s 265ms/step - loss: 0.1814 - val_loss: 3247762.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 153/5000\n",
      "1/1 [==============================] - 0s 262ms/step - loss: 0.1812 - val_loss: 3197087.0000\n",
      "Epoch 154/5000\n",
      "1/1 [==============================] - 0s 262ms/step - loss: 0.1814 - val_loss: 3137078.2500\n",
      "Epoch 155/5000\n",
      "1/1 [==============================] - 0s 260ms/step - loss: 0.1814 - val_loss: 3050795.2500\n",
      "Epoch 156/5000\n",
      "1/1 [==============================] - 0s 259ms/step - loss: 0.1814 - val_loss: 2970632.0000\n",
      "Epoch 157/5000\n",
      "1/1 [==============================] - 0s 261ms/step - loss: 0.1812 - val_loss: 2865847.5000\n",
      "Epoch 158/5000\n",
      "1/1 [==============================] - 0s 259ms/step - loss: 0.1815 - val_loss: 2828727.0000\n",
      "Epoch 159/5000\n",
      "1/1 [==============================] - 0s 260ms/step - loss: 0.1818 - val_loss: 2728219.7500\n",
      "Epoch 160/5000\n",
      "1/1 [==============================] - 0s 264ms/step - loss: 0.1820 - val_loss: 2645036.7500\n",
      "Epoch 161/5000\n",
      "1/1 [==============================] - 0s 261ms/step - loss: 0.1811 - val_loss: 2553545.5000\n",
      "Epoch 162/5000\n",
      "1/1 [==============================] - 0s 260ms/step - loss: 0.1807 - val_loss: 2456643.5000\n",
      "Epoch 163/5000\n",
      "1/1 [==============================] - 0s 262ms/step - loss: 0.1805 - val_loss: 2430066.5000\n",
      "Epoch 164/5000\n",
      "1/1 [==============================] - 0s 260ms/step - loss: 0.1805 - val_loss: 2327735.0000\n",
      "Epoch 165/5000\n",
      "1/1 [==============================] - 0s 259ms/step - loss: 0.1805 - val_loss: 2331227.0000\n",
      "Epoch 166/5000\n",
      "1/1 [==============================] - 0s 261ms/step - loss: 0.1815 - val_loss: 2230600.0000\n",
      "Epoch 167/5000\n",
      "1/1 [==============================] - 0s 260ms/step - loss: 0.1810 - val_loss: 2213873.5000\n",
      "Epoch 168/5000\n",
      "1/1 [==============================] - 0s 260ms/step - loss: 0.1806 - val_loss: 2133034.5000\n",
      "Epoch 169/5000\n",
      "1/1 [==============================] - 0s 260ms/step - loss: 0.1801 - val_loss: 2077666.3750\n",
      "Epoch 170/5000\n",
      "1/1 [==============================] - 0s 260ms/step - loss: 0.1802 - val_loss: 2030630.2500\n",
      "Epoch 171/5000\n",
      "1/1 [==============================] - 0s 260ms/step - loss: 0.1798 - val_loss: 1970065.3750\n",
      "Epoch 172/5000\n",
      "1/1 [==============================] - 0s 261ms/step - loss: 0.1799 - val_loss: 1933827.0000\n",
      "Epoch 173/5000\n",
      "1/1 [==============================] - 0s 264ms/step - loss: 0.1796 - val_loss: 1874191.2500\n",
      "Epoch 174/5000\n",
      "1/1 [==============================] - 0s 260ms/step - loss: 0.1795 - val_loss: 1843522.0000\n",
      "Epoch 175/5000\n",
      "1/1 [==============================] - 0s 260ms/step - loss: 0.1795 - val_loss: 1766425.6250\n",
      "Epoch 176/5000\n",
      "1/1 [==============================] - 0s 260ms/step - loss: 0.1798 - val_loss: 1743195.0000\n",
      "Epoch 177/5000\n",
      "1/1 [==============================] - 0s 260ms/step - loss: 0.1806 - val_loss: 1680199.0000\n",
      "Epoch 178/5000\n",
      "1/1 [==============================] - 0s 259ms/step - loss: 0.1812 - val_loss: 1689291.1250\n",
      "Epoch 179/5000\n",
      "1/1 [==============================] - 0s 261ms/step - loss: 0.1811 - val_loss: 1629139.7500\n",
      "Epoch 180/5000\n",
      "1/1 [==============================] - 0s 261ms/step - loss: 0.1805 - val_loss: 1618356.5000\n",
      "Epoch 181/5000\n",
      "1/1 [==============================] - 0s 259ms/step - loss: 0.1803 - val_loss: 1570945.7500\n",
      "Epoch 182/5000\n",
      "1/1 [==============================] - 0s 260ms/step - loss: 0.1801 - val_loss: 1546061.8750\n",
      "Epoch 183/5000\n",
      "1/1 [==============================] - 0s 259ms/step - loss: 0.1802 - val_loss: 1518802.3750\n",
      "Epoch 184/5000\n",
      "1/1 [==============================] - 0s 260ms/step - loss: 0.1805 - val_loss: 1436174.5000\n",
      "Epoch 185/5000\n",
      "1/1 [==============================] - 0s 261ms/step - loss: 0.1806 - val_loss: 1441668.8750\n",
      "Epoch 186/5000\n",
      "1/1 [==============================] - 0s 262ms/step - loss: 0.1807 - val_loss: 1363470.5000\n",
      "Epoch 187/5000\n",
      "1/1 [==============================] - 0s 270ms/step - loss: 0.1801 - val_loss: 1358117.5000\n",
      "Epoch 188/5000\n",
      "1/1 [==============================] - 0s 261ms/step - loss: 0.1799 - val_loss: 1299832.1250\n",
      "Epoch 189/5000\n",
      "1/1 [==============================] - 0s 265ms/step - loss: 0.1795 - val_loss: 1281080.2500\n",
      "Epoch 190/5000\n",
      "1/1 [==============================] - 0s 266ms/step - loss: 0.1796 - val_loss: 1235537.0000\n",
      "Epoch 191/5000\n",
      "1/1 [==============================] - 0s 259ms/step - loss: 0.1799 - val_loss: 1217012.5000\n",
      "Epoch 192/5000\n",
      "1/1 [==============================] - 0s 260ms/step - loss: 0.1801 - val_loss: 1174256.2500\n",
      "Epoch 193/5000\n",
      "1/1 [==============================] - 0s 260ms/step - loss: 0.1802 - val_loss: 1165205.0000\n",
      "Epoch 194/5000\n",
      "1/1 [==============================] - 0s 262ms/step - loss: 0.1813 - val_loss: 1124628.1250\n",
      "Epoch 195/5000\n",
      "1/1 [==============================] - 0s 260ms/step - loss: 0.1814 - val_loss: 1124065.3750\n",
      "Epoch 196/5000\n",
      "1/1 [==============================] - 0s 260ms/step - loss: 0.1810 - val_loss: 1082810.6250\n",
      "Epoch 197/5000\n",
      "1/1 [==============================] - 0s 267ms/step - loss: 0.1802 - val_loss: 1044899.4375\n",
      "Epoch 198/5000\n",
      "1/1 [==============================] - 0s 262ms/step - loss: 0.1800 - val_loss: 1029715.6250\n",
      "Epoch 199/5000\n",
      "1/1 [==============================] - 0s 262ms/step - loss: 0.1798 - val_loss: 979125.1250\n",
      "Epoch 200/5000\n",
      "1/1 [==============================] - 0s 264ms/step - loss: 0.1801 - val_loss: 965163.7500\n",
      "Epoch 201/5000\n",
      "1/1 [==============================] - 0s 262ms/step - loss: 0.1802 - val_loss: 923229.1250\n",
      "Epoch 202/5000\n",
      "1/1 [==============================] - 0s 260ms/step - loss: 0.1820 - val_loss: 895867.3125\n",
      "Epoch 203/5000\n",
      "1/1 [==============================] - 0s 263ms/step - loss: 0.1813 - val_loss: 889288.4375\n",
      "Epoch 204/5000\n",
      "1/1 [==============================] - 0s 264ms/step - loss: 0.1803 - val_loss: 857100.5000\n",
      "Epoch 205/5000\n",
      "1/1 [==============================] - 0s 260ms/step - loss: 0.1798 - val_loss: 846564.0625\n",
      "Epoch 206/5000\n",
      "1/1 [==============================] - 0s 261ms/step - loss: 0.1797 - val_loss: 818083.9375\n",
      "Epoch 207/5000\n",
      "1/1 [==============================] - 0s 262ms/step - loss: 0.1798 - val_loss: 804074.1875\n",
      "Epoch 208/5000\n",
      "1/1 [==============================] - 0s 260ms/step - loss: 0.1807 - val_loss: 794741.6250\n",
      "Epoch 209/5000\n",
      "1/1 [==============================] - 0s 261ms/step - loss: 0.1804 - val_loss: 774031.3750\n",
      "Epoch 210/5000\n",
      "1/1 [==============================] - 0s 262ms/step - loss: 0.1802 - val_loss: 760454.3125\n",
      "Epoch 211/5000\n",
      "1/1 [==============================] - 0s 260ms/step - loss: 0.1797 - val_loss: 736083.6875\n",
      "Epoch 212/5000\n",
      "1/1 [==============================] - 0s 260ms/step - loss: 0.1797 - val_loss: 728795.5625\n",
      "Epoch 213/5000\n",
      "1/1 [==============================] - 0s 260ms/step - loss: 0.1800 - val_loss: 696738.3750\n",
      "Epoch 214/5000\n",
      "1/1 [==============================] - 0s 259ms/step - loss: 0.1810 - val_loss: 682882.9375\n",
      "Epoch 215/5000\n",
      "1/1 [==============================] - 0s 261ms/step - loss: 0.1805 - val_loss: 671788.3750\n",
      "Epoch 216/5000\n",
      "1/1 [==============================] - 0s 261ms/step - loss: 0.1800 - val_loss: 649930.1875\n",
      "Epoch 217/5000\n",
      "1/1 [==============================] - 0s 265ms/step - loss: 0.1798 - val_loss: 644742.5625\n",
      "Epoch 218/5000\n",
      "1/1 [==============================] - 0s 263ms/step - loss: 0.1796 - val_loss: 617142.9375\n",
      "Epoch 219/5000\n",
      "1/1 [==============================] - 0s 263ms/step - loss: 0.1791 - val_loss: 606098.9375\n",
      "Epoch 220/5000\n",
      "1/1 [==============================] - 0s 262ms/step - loss: 0.1789 - val_loss: 592089.1875\n",
      "Epoch 221/5000\n",
      "1/1 [==============================] - 0s 260ms/step - loss: 0.1788 - val_loss: 571753.3750\n",
      "Epoch 222/5000\n",
      "1/1 [==============================] - 0s 260ms/step - loss: 0.1795 - val_loss: 564769.0000\n",
      "Epoch 223/5000\n",
      "1/1 [==============================] - 0s 260ms/step - loss: 0.1817 - val_loss: 544610.6875\n",
      "Epoch 224/5000\n",
      "1/1 [==============================] - 0s 260ms/step - loss: 0.1818 - val_loss: 520744.0938\n",
      "Epoch 225/5000\n",
      "1/1 [==============================] - 0s 260ms/step - loss: 0.1817 - val_loss: 506073.0938\n",
      "Epoch 226/5000\n",
      "1/1 [==============================] - 0s 264ms/step - loss: 0.1810 - val_loss: 494306.5000\n",
      "Epoch 227/5000\n",
      "1/1 [==============================] - 0s 269ms/step - loss: 0.1798 - val_loss: 484615.0625\n",
      "Epoch 228/5000\n",
      "1/1 [==============================] - 0s 264ms/step - loss: 0.1793 - val_loss: 472814.9062\n",
      "Epoch 229/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 260ms/step - loss: 0.1789 - val_loss: 455159.4062\n",
      "Epoch 230/5000\n",
      "1/1 [==============================] - 0s 264ms/step - loss: 0.1788 - val_loss: 447925.3125\n",
      "Epoch 231/5000\n",
      "1/1 [==============================] - 0s 261ms/step - loss: 0.1787 - val_loss: 435001.1875\n",
      "Epoch 232/5000\n",
      "1/1 [==============================] - 0s 261ms/step - loss: 0.1791 - val_loss: 438867.8125\n",
      "Epoch 233/5000\n",
      "1/1 [==============================] - 0s 261ms/step - loss: 0.1791 - val_loss: 418392.2188\n",
      "Epoch 234/5000\n",
      "1/1 [==============================] - 0s 259ms/step - loss: 0.1804 - val_loss: 419291.4688\n",
      "Epoch 235/5000\n",
      "1/1 [==============================] - 0s 259ms/step - loss: 0.1801 - val_loss: 398081.9688\n",
      "Epoch 236/5000\n",
      "1/1 [==============================] - 0s 260ms/step - loss: 0.1801 - val_loss: 392990.2812\n",
      "Epoch 237/5000\n",
      "1/1 [==============================] - 0s 267ms/step - loss: 0.1791 - val_loss: 373008.0000\n",
      "Epoch 238/5000\n",
      "1/1 [==============================] - 0s 260ms/step - loss: 0.1789 - val_loss: 366732.6250\n",
      "Epoch 239/5000\n",
      "1/1 [==============================] - 0s 261ms/step - loss: 0.1786 - val_loss: 348156.5312\n",
      "Epoch 240/5000\n",
      "1/1 [==============================] - 0s 262ms/step - loss: 0.1787 - val_loss: 339700.3438\n",
      "Epoch 241/5000\n",
      "1/1 [==============================] - 0s 260ms/step - loss: 0.1784 - val_loss: 328414.3438\n",
      "Epoch 242/5000\n",
      "1/1 [==============================] - 0s 262ms/step - loss: 0.1786 - val_loss: 321487.4688\n",
      "Epoch 243/5000\n",
      "1/1 [==============================] - 0s 267ms/step - loss: 0.1795 - val_loss: 309016.5625\n",
      "Epoch 244/5000\n",
      "1/1 [==============================] - 0s 263ms/step - loss: 0.1797 - val_loss: 292887.5312\n",
      "Epoch 245/5000\n",
      "1/1 [==============================] - 0s 260ms/step - loss: 0.1805 - val_loss: 282304.4375\n",
      "Epoch 246/5000\n",
      "1/1 [==============================] - 0s 260ms/step - loss: 0.1795 - val_loss: 279946.8125\n",
      "Epoch 247/5000\n",
      "1/1 [==============================] - 0s 270ms/step - loss: 0.1788 - val_loss: 272297.2500\n",
      "Epoch 248/5000\n",
      "1/1 [==============================] - 0s 265ms/step - loss: 0.1781 - val_loss: 269793.0312\n",
      "Epoch 249/5000\n",
      "1/1 [==============================] - 0s 263ms/step - loss: 0.1779 - val_loss: 262498.3125\n",
      "Epoch 250/5000\n",
      "1/1 [==============================] - 0s 275ms/step - loss: 0.1780 - val_loss: 260708.2344\n",
      "Epoch 251/5000\n",
      "1/1 [==============================] - 0s 259ms/step - loss: 0.1788 - val_loss: 260201.4844\n",
      "Epoch 252/5000\n",
      "1/1 [==============================] - 0s 263ms/step - loss: 0.1786 - val_loss: 256738.1094\n",
      "Epoch 253/5000\n",
      "1/1 [==============================] - 0s 268ms/step - loss: 0.1789 - val_loss: 254549.9688\n",
      "Epoch 254/5000\n",
      "1/1 [==============================] - 0s 266ms/step - loss: 0.1785 - val_loss: 252037.8906\n",
      "Epoch 255/5000\n",
      "1/1 [==============================] - 0s 265ms/step - loss: 0.1782 - val_loss: 243682.6562\n",
      "Epoch 256/5000\n",
      "1/1 [==============================] - 0s 265ms/step - loss: 0.1786 - val_loss: 235844.9688\n",
      "Epoch 257/5000\n",
      "1/1 [==============================] - 0s 262ms/step - loss: 0.1799 - val_loss: 230634.0312\n",
      "Epoch 258/5000\n",
      "1/1 [==============================] - 0s 261ms/step - loss: 0.1797 - val_loss: 228909.5781\n",
      "Epoch 259/5000\n",
      "1/1 [==============================] - 0s 260ms/step - loss: 0.1794 - val_loss: 213484.7812\n",
      "Epoch 260/5000\n",
      "1/1 [==============================] - 0s 261ms/step - loss: 0.1803 - val_loss: 222021.6250\n",
      "Epoch 261/5000\n",
      "1/1 [==============================] - 0s 260ms/step - loss: 0.1802 - val_loss: 208669.7031\n",
      "Epoch 262/5000\n",
      "1/1 [==============================] - 0s 261ms/step - loss: 0.1800 - val_loss: 210201.3438\n",
      "Epoch 263/5000\n",
      "1/1 [==============================] - 0s 259ms/step - loss: 0.1804 - val_loss: 202513.7031\n",
      "Epoch 264/5000\n",
      "1/1 [==============================] - 0s 261ms/step - loss: 0.1803 - val_loss: 198146.0312\n",
      "Epoch 265/5000\n",
      "1/1 [==============================] - 0s 259ms/step - loss: 0.1809 - val_loss: 196296.1094\n",
      "Epoch 266/5000\n",
      "1/1 [==============================] - 0s 261ms/step - loss: 0.1803 - val_loss: 194333.3281\n",
      "Epoch 267/5000\n",
      "1/1 [==============================] - 0s 260ms/step - loss: 0.1800 - val_loss: 190719.7656\n",
      "Epoch 268/5000\n",
      "1/1 [==============================] - 0s 261ms/step - loss: 0.1802 - val_loss: 189174.8281\n",
      "Epoch 269/5000\n",
      "1/1 [==============================] - 0s 261ms/step - loss: 0.1805 - val_loss: 188396.7969\n",
      "Epoch 270/5000\n",
      "1/1 [==============================] - 0s 263ms/step - loss: 0.1806 - val_loss: 187788.6094\n",
      "Epoch 271/5000\n",
      "1/1 [==============================] - 0s 260ms/step - loss: 0.1808 - val_loss: 185150.6406\n",
      "Epoch 272/5000\n",
      "1/1 [==============================] - 0s 269ms/step - loss: 0.1812 - val_loss: 179768.5000\n",
      "Epoch 273/5000\n",
      "1/1 [==============================] - 0s 268ms/step - loss: 0.1807 - val_loss: 173758.8438\n",
      "Epoch 274/5000\n",
      "1/1 [==============================] - 0s 263ms/step - loss: 0.1798 - val_loss: 171077.0312\n",
      "Epoch 275/5000\n",
      "1/1 [==============================] - 0s 264ms/step - loss: 0.1793 - val_loss: 173171.4844\n",
      "Epoch 276/5000\n",
      "1/1 [==============================] - 0s 268ms/step - loss: 0.1796 - val_loss: 168417.6562\n",
      "Epoch 277/5000\n",
      "1/1 [==============================] - 0s 264ms/step - loss: 0.1821 - val_loss: 169477.0469\n",
      "Epoch 278/5000\n",
      "1/1 [==============================] - 0s 260ms/step - loss: 0.1814 - val_loss: 173130.5469\n",
      "Epoch 279/5000\n",
      "1/1 [==============================] - 0s 261ms/step - loss: 0.1801 - val_loss: 166698.6875\n",
      "Epoch 280/5000\n",
      "1/1 [==============================] - 0s 260ms/step - loss: 0.1791 - val_loss: 166221.0938\n",
      "Epoch 281/5000\n",
      "1/1 [==============================] - 0s 260ms/step - loss: 0.1784 - val_loss: 160409.0469\n",
      "Epoch 282/5000\n",
      "1/1 [==============================] - 0s 261ms/step - loss: 0.1781 - val_loss: 161745.3438\n",
      "Epoch 283/5000\n",
      "1/1 [==============================] - 0s 260ms/step - loss: 0.1780 - val_loss: 155332.3906\n",
      "Epoch 284/5000\n",
      "1/1 [==============================] - 0s 259ms/step - loss: 0.1778 - val_loss: 157180.2500\n",
      "Epoch 285/5000\n",
      "1/1 [==============================] - 0s 262ms/step - loss: 0.1781 - val_loss: 154828.2969\n",
      "Epoch 286/5000\n",
      "1/1 [==============================] - 0s 263ms/step - loss: 0.1781 - val_loss: 154162.8906\n",
      "Epoch 287/5000\n",
      "1/1 [==============================] - 0s 259ms/step - loss: 0.1787 - val_loss: 154904.9688\n",
      "Epoch 288/5000\n",
      "1/1 [==============================] - 0s 261ms/step - loss: 0.1784 - val_loss: 150499.7969\n",
      "Epoch 289/5000\n",
      "1/1 [==============================] - 0s 260ms/step - loss: 0.1782 - val_loss: 152121.5469\n",
      "Epoch 290/5000\n",
      "1/1 [==============================] - 0s 260ms/step - loss: 0.1777 - val_loss: 151287.2031\n",
      "Epoch 291/5000\n",
      "1/1 [==============================] - 0s 259ms/step - loss: 0.1775 - val_loss: 149274.5625\n",
      "Epoch 292/5000\n",
      "1/1 [==============================] - 0s 262ms/step - loss: 0.1775 - val_loss: 150882.2969\n",
      "Epoch 293/5000\n",
      "1/1 [==============================] - 0s 268ms/step - loss: 0.1776 - val_loss: 149677.0469\n",
      "Epoch 294/5000\n",
      "1/1 [==============================] - 0s 261ms/step - loss: 0.1778 - val_loss: 147738.1875\n",
      "Epoch 295/5000\n",
      "1/1 [==============================] - 0s 263ms/step - loss: 0.1780 - val_loss: 145451.5000\n",
      "Epoch 296/5000\n",
      "1/1 [==============================] - 0s 265ms/step - loss: 0.1779 - val_loss: 147014.4688\n",
      "Epoch 297/5000\n",
      "1/1 [==============================] - 0s 260ms/step - loss: 0.1779 - val_loss: 147549.7500\n",
      "Epoch 298/5000\n",
      "1/1 [==============================] - 0s 261ms/step - loss: 0.1778 - val_loss: 146489.6250\n",
      "Epoch 299/5000\n",
      "1/1 [==============================] - 0s 263ms/step - loss: 0.1778 - val_loss: 146363.2812\n",
      "Epoch 300/5000\n",
      "1/1 [==============================] - 0s 260ms/step - loss: 0.1776 - val_loss: 143440.4062\n",
      "Epoch 301/5000\n",
      "1/1 [==============================] - 0s 262ms/step - loss: 0.1780 - val_loss: 146127.3594\n",
      "Epoch 302/5000\n",
      "1/1 [==============================] - 0s 263ms/step - loss: 0.1777 - val_loss: 142896.8125\n",
      "Epoch 303/5000\n",
      "1/1 [==============================] - 0s 270ms/step - loss: 0.1777 - val_loss: 145437.5469\n",
      "Epoch 304/5000\n",
      "1/1 [==============================] - 0s 261ms/step - loss: 0.1775 - val_loss: 142093.4531\n",
      "Epoch 305/5000\n",
      "1/1 [==============================] - 0s 261ms/step - loss: 0.1775 - val_loss: 145216.7031\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 306/5000\n",
      "1/1 [==============================] - 0s 267ms/step - loss: 0.1775 - val_loss: 143023.4062\n",
      "Epoch 307/5000\n",
      "1/1 [==============================] - 0s 268ms/step - loss: 0.1780 - val_loss: 146757.5781\n",
      "Epoch 308/5000\n",
      "1/1 [==============================] - 0s 267ms/step - loss: 0.1780 - val_loss: 144075.6406\n",
      "Epoch 309/5000\n",
      "1/1 [==============================] - 0s 264ms/step - loss: 0.1781 - val_loss: 147243.3438\n",
      "Epoch 310/5000\n",
      "1/1 [==============================] - 0s 262ms/step - loss: 0.1777 - val_loss: 147821.5000\n",
      "Epoch 311/5000\n",
      "1/1 [==============================] - 0s 262ms/step - loss: 0.1775 - val_loss: 148548.9531\n",
      "Epoch 312/5000\n",
      "1/1 [==============================] - 0s 262ms/step - loss: 0.1772 - val_loss: 148383.8125\n",
      "Epoch 313/5000\n",
      "1/1 [==============================] - 0s 260ms/step - loss: 0.1775 - val_loss: 147620.1719\n",
      "Epoch 314/5000\n",
      "1/1 [==============================] - 0s 259ms/step - loss: 0.1776 - val_loss: 145503.5625\n",
      "Epoch 315/5000\n",
      "1/1 [==============================] - 0s 260ms/step - loss: 0.1782 - val_loss: 146626.5469\n",
      "\n",
      "Epoch 00315: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
      "Epoch 316/5000\n",
      "1/1 [==============================] - 0s 261ms/step - loss: 0.1780 - val_loss: 147302.7031\n",
      "Epoch 317/5000\n",
      "1/1 [==============================] - 0s 262ms/step - loss: 0.1774 - val_loss: 147711.0781\n",
      "Epoch 318/5000\n",
      "1/1 [==============================] - 0s 264ms/step - loss: 0.1771 - val_loss: 148420.3594\n",
      "Epoch 319/5000\n",
      "1/1 [==============================] - 0s 269ms/step - loss: 0.1768 - val_loss: 149127.6406\n",
      "Epoch 320/5000\n",
      "1/1 [==============================] - 0s 264ms/step - loss: 0.1767 - val_loss: 150352.2188\n",
      "Epoch 321/5000\n",
      "1/1 [==============================] - 0s 261ms/step - loss: 0.1766 - val_loss: 151616.5469\n",
      "Epoch 322/5000\n",
      "1/1 [==============================] - 0s 262ms/step - loss: 0.1765 - val_loss: 153026.1406\n",
      "Epoch 323/5000\n",
      "1/1 [==============================] - 0s 261ms/step - loss: 0.1764 - val_loss: 154252.7969\n",
      "Epoch 324/5000\n",
      "1/1 [==============================] - 0s 260ms/step - loss: 0.1764 - val_loss: 155471.4531\n",
      "Epoch 325/5000\n",
      "1/1 [==============================] - 0s 260ms/step - loss: 0.1763 - val_loss: 156393.7969\n",
      "\n",
      "Epoch 00325: ReduceLROnPlateau reducing learning rate to 9.999999310821295e-05.\n",
      "Epoch 326/5000\n",
      "1/1 [==============================] - 0s 259ms/step - loss: 0.1763 - val_loss: 157409.2500\n",
      "Epoch 327/5000\n",
      "1/1 [==============================] - 0s 260ms/step - loss: 0.1763 - val_loss: 158441.7188\n",
      "Epoch 328/5000\n",
      "1/1 [==============================] - 0s 263ms/step - loss: 0.1763 - val_loss: 159411.2656\n",
      "Epoch 329/5000\n",
      "1/1 [==============================] - 0s 271ms/step - loss: 0.1763 - val_loss: 160164.3906\n",
      "Epoch 330/5000\n",
      "1/1 [==============================] - 0s 263ms/step - loss: 0.1763 - val_loss: 160235.7969\n",
      "Epoch 331/5000\n",
      "1/1 [==============================] - 0s 262ms/step - loss: 0.1763 - val_loss: 159953.0625\n",
      "Epoch 332/5000\n",
      "1/1 [==============================] - 0s 265ms/step - loss: 0.1763 - val_loss: 159752.8750\n",
      "Epoch 333/5000\n",
      "1/1 [==============================] - 0s 261ms/step - loss: 0.1763 - val_loss: 159705.7031\n",
      "Epoch 334/5000\n",
      "1/1 [==============================] - 0s 260ms/step - loss: 0.1763 - val_loss: 159506.9219\n",
      "Epoch 335/5000\n",
      "1/1 [==============================] - 0s 260ms/step - loss: 0.1763 - val_loss: 159216.4375\n",
      "\n",
      "Epoch 00335: ReduceLROnPlateau reducing learning rate to 9.999999019782991e-06.\n",
      "Epoch 336/5000\n",
      "1/1 [==============================] - 0s 260ms/step - loss: 0.1763 - val_loss: 159297.9219\n",
      "Epoch 337/5000\n",
      "1/1 [==============================] - 0s 260ms/step - loss: 0.1763 - val_loss: 159398.5000\n",
      "Epoch 338/5000\n",
      "1/1 [==============================] - 0s 260ms/step - loss: 0.1763 - val_loss: 159466.7969\n",
      "Epoch 339/5000\n",
      "1/1 [==============================] - 0s 268ms/step - loss: 0.1763 - val_loss: 159517.6719\n",
      "Epoch 340/5000\n",
      "1/1 [==============================] - 0s 262ms/step - loss: 0.1763 - val_loss: 159336.6875\n",
      "Epoch 341/5000\n",
      "1/1 [==============================] - 0s 263ms/step - loss: 0.1763 - val_loss: 159027.5625\n",
      "Epoch 342/5000\n",
      "1/1 [==============================] - 0s 262ms/step - loss: 0.1763 - val_loss: 158602.1094\n",
      "Epoch 343/5000\n",
      "1/1 [==============================] - 0s 259ms/step - loss: 0.1763 - val_loss: 157980.9688\n",
      "Epoch 344/5000\n",
      "1/1 [==============================] - 0s 261ms/step - loss: 0.1763 - val_loss: 157115.9062\n",
      "Epoch 345/5000\n",
      "1/1 [==============================] - 0s 260ms/step - loss: 0.1763 - val_loss: 156268.4531\n",
      "\n",
      "Epoch 00345: ReduceLROnPlateau reducing learning rate to 9.99999883788405e-07.\n",
      "Epoch 346/5000\n",
      "1/1 [==============================] - 0s 260ms/step - loss: 0.1763 - val_loss: 155220.3750\n",
      "Epoch 347/5000\n",
      "1/1 [==============================] - 0s 260ms/step - loss: 0.1763 - val_loss: 154141.0469\n",
      "Epoch 348/5000\n",
      "1/1 [==============================] - 0s 261ms/step - loss: 0.1763 - val_loss: 153175.9219\n",
      "Epoch 349/5000\n",
      "1/1 [==============================] - 0s 262ms/step - loss: 0.1763 - val_loss: 152555.0469\n",
      "Epoch 350/5000\n",
      "1/1 [==============================] - 0s 259ms/step - loss: 0.1763 - val_loss: 152050.7656\n",
      "Epoch 351/5000\n",
      "1/1 [==============================] - 0s 261ms/step - loss: 0.1763 - val_loss: 151565.6875\n",
      "Epoch 352/5000\n",
      "1/1 [==============================] - 0s 262ms/step - loss: 0.1763 - val_loss: 151034.4844\n",
      "Epoch 353/5000\n",
      "1/1 [==============================] - 0s 261ms/step - loss: 0.1763 - val_loss: 150420.7656\n",
      "Epoch 354/5000\n",
      "1/1 [==============================] - 0s 260ms/step - loss: 0.1763 - val_loss: 149826.8281\n",
      "Epoch 355/5000\n",
      "1/1 [==============================] - 0s 260ms/step - loss: 0.1763 - val_loss: 149147.4375\n",
      "\n",
      "Epoch 00355: ReduceLROnPlateau reducing learning rate to 9.99999883788405e-08.\n",
      "Epoch 356/5000\n",
      "1/1 [==============================] - 0s 260ms/step - loss: 0.1763 - val_loss: 148674.6719\n",
      "Epoch 357/5000\n",
      "1/1 [==============================] - 0s 259ms/step - loss: 0.1763 - val_loss: 148411.0938\n",
      "Epoch 358/5000\n",
      "1/1 [==============================] - 0s 261ms/step - loss: 0.1763 - val_loss: 148240.4531\n",
      "Epoch 359/5000\n",
      "1/1 [==============================] - 0s 266ms/step - loss: 0.1763 - val_loss: 148329.6562\n",
      "Epoch 360/5000\n",
      "1/1 [==============================] - 0s 262ms/step - loss: 0.1763 - val_loss: 148622.7656\n",
      "Epoch 361/5000\n",
      "1/1 [==============================] - 0s 264ms/step - loss: 0.1763 - val_loss: 148877.3750\n",
      "Epoch 362/5000\n",
      "1/1 [==============================] - 0s 261ms/step - loss: 0.1763 - val_loss: 148940.1562\n",
      "Epoch 363/5000\n",
      "1/1 [==============================] - 0s 261ms/step - loss: 0.1763 - val_loss: 149077.9219\n",
      "Epoch 364/5000\n",
      "1/1 [==============================] - 0s 260ms/step - loss: 0.1763 - val_loss: 149303.7969\n",
      "Epoch 365/5000\n",
      "1/1 [==============================] - 0s 267ms/step - loss: 0.1763 - val_loss: 149334.7031\n",
      "\n",
      "Epoch 00365: ReduceLROnPlateau reducing learning rate to 9.999998695775504e-09.\n",
      "Epoch 366/5000\n",
      "1/1 [==============================] - 0s 260ms/step - loss: 0.1763 - val_loss: 149260.6875\n",
      "Epoch 367/5000\n",
      "1/1 [==============================] - 0s 260ms/step - loss: 0.1763 - val_loss: 149209.1875\n",
      "Epoch 368/5000\n",
      "1/1 [==============================] - 0s 261ms/step - loss: 0.1763 - val_loss: 149225.5000\n",
      "Epoch 369/5000\n",
      "1/1 [==============================] - 0s 260ms/step - loss: 0.1763 - val_loss: 149271.6406\n",
      "Epoch 370/5000\n",
      "1/1 [==============================] - 0s 261ms/step - loss: 0.1763 - val_loss: 149351.2188\n",
      "Epoch 371/5000\n",
      "1/1 [==============================] - 0s 272ms/step - loss: 0.1763 - val_loss: 149542.1094\n",
      "Epoch 372/5000\n",
      "1/1 [==============================] - 0s 269ms/step - loss: 0.1763 - val_loss: 149879.5156\n",
      "Epoch 373/5000\n",
      "1/1 [==============================] - 0s 261ms/step - loss: 0.1763 - val_loss: 150210.5312\n",
      "Epoch 374/5000\n",
      "1/1 [==============================] - 0s 270ms/step - loss: 0.1763 - val_loss: 150500.0938\n",
      "Epoch 375/5000\n",
      "1/1 [==============================] - 0s 287ms/step - loss: 0.1763 - val_loss: 150807.2500\n",
      "\n",
      "Epoch 00375: ReduceLROnPlateau reducing learning rate to 9.99999905104687e-10.\n",
      "Epoch 376/5000\n",
      "1/1 [==============================] - 0s 271ms/step - loss: 0.1763 - val_loss: 151202.8438\n",
      "Epoch 377/5000\n",
      "1/1 [==============================] - 0s 270ms/step - loss: 0.1763 - val_loss: 151675.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 378/5000\n",
      "1/1 [==============================] - 0s 265ms/step - loss: 0.1763 - val_loss: 152190.0000\n",
      "Epoch 379/5000\n",
      "1/1 [==============================] - 0s 261ms/step - loss: 0.1763 - val_loss: 152856.2500\n",
      "Epoch 380/5000\n",
      "1/1 [==============================] - 0s 259ms/step - loss: 0.1763 - val_loss: 153579.7344\n",
      "Epoch 381/5000\n",
      "1/1 [==============================] - 0s 261ms/step - loss: 0.1763 - val_loss: 154310.6875\n",
      "Epoch 382/5000\n",
      "1/1 [==============================] - 0s 261ms/step - loss: 0.1763 - val_loss: 154866.2344\n",
      "Epoch 383/5000\n",
      "1/1 [==============================] - 0s 261ms/step - loss: 0.1763 - val_loss: 155207.2031\n",
      "Epoch 384/5000\n",
      "1/1 [==============================] - 0s 265ms/step - loss: 0.1763 - val_loss: 155259.7031\n",
      "Epoch 385/5000\n",
      "1/1 [==============================] - 0s 260ms/step - loss: 0.1763 - val_loss: 155210.2031\n",
      "\n",
      "Epoch 00385: ReduceLROnPlateau reducing learning rate to 9.999998606957661e-11.\n",
      "Epoch 386/5000\n",
      "1/1 [==============================] - 0s 265ms/step - loss: 0.1763 - val_loss: 155108.8906\n",
      "Epoch 387/5000\n",
      "1/1 [==============================] - 0s 262ms/step - loss: 0.1763 - val_loss: 155128.6562\n",
      "Epoch 388/5000\n",
      "1/1 [==============================] - 0s 261ms/step - loss: 0.1763 - val_loss: 155351.4062\n",
      "Epoch 389/5000\n",
      "1/1 [==============================] - 0s 261ms/step - loss: 0.1763 - val_loss: 155548.2812\n",
      "Epoch 390/5000\n",
      "1/1 [==============================] - 0s 261ms/step - loss: 0.1763 - val_loss: 155623.0938\n",
      "Epoch 391/5000\n",
      "1/1 [==============================] - 0s 260ms/step - loss: 0.1763 - val_loss: 155683.7656\n",
      "Epoch 392/5000\n",
      "1/1 [==============================] - 0s 261ms/step - loss: 0.1763 - val_loss: 155770.1406\n",
      "Epoch 393/5000\n",
      "1/1 [==============================] - 0s 264ms/step - loss: 0.1763 - val_loss: 155807.1719\n",
      "Epoch 394/5000\n",
      "1/1 [==============================] - 0s 264ms/step - loss: 0.1763 - val_loss: 155863.0469\n",
      "Epoch 395/5000\n",
      "1/1 [==============================] - 0s 264ms/step - loss: 0.1763 - val_loss: 155861.7188\n",
      "\n",
      "Epoch 00395: ReduceLROnPlateau reducing learning rate to 9.99999874573554e-12.\n",
      "Epoch 396/5000\n",
      "1/1 [==============================] - 0s 273ms/step - loss: 0.1763 - val_loss: 156011.0312\n",
      "Epoch 397/5000\n",
      "1/1 [==============================] - 0s 275ms/step - loss: 0.1763 - val_loss: 156189.0781\n",
      "Epoch 398/5000\n",
      "1/1 [==============================] - 0s 273ms/step - loss: 0.1763 - val_loss: 156364.1719\n",
      "Epoch 399/5000\n",
      "1/1 [==============================] - 0s 282ms/step - loss: 0.1763 - val_loss: 156614.4219\n",
      "Epoch 400/5000\n",
      "1/1 [==============================] - 0s 288ms/step - loss: 0.1763 - val_loss: 156900.2344\n",
      "Epoch 401/5000\n",
      "1/1 [==============================] - 0s 289ms/step - loss: 0.1763 - val_loss: 157255.5469\n",
      "Epoch 402/5000\n",
      "1/1 [==============================] - 0s 290ms/step - loss: 0.1763 - val_loss: 157762.1562\n",
      "Epoch 403/5000\n",
      "1/1 [==============================] - 0s 273ms/step - loss: 0.1763 - val_loss: 158346.8281\n",
      "Epoch 404/5000\n",
      "1/1 [==============================] - 0s 266ms/step - loss: 0.1763 - val_loss: 158968.9375\n",
      "Epoch 405/5000\n",
      "1/1 [==============================] - 0s 264ms/step - loss: 0.1763 - val_loss: 159660.4375\n",
      "\n",
      "Epoch 00405: ReduceLROnPlateau reducing learning rate to 9.999999092680235e-13.\n",
      "Epoch 406/5000\n",
      "1/1 [==============================] - 0s 262ms/step - loss: 0.1763 - val_loss: 160324.9375\n",
      "Epoch 407/5000\n",
      "1/1 [==============================] - 0s 262ms/step - loss: 0.1763 - val_loss: 160924.4531\n",
      "Epoch 408/5000\n",
      "1/1 [==============================] - 0s 262ms/step - loss: 0.1763 - val_loss: 161497.5469\n",
      "Epoch 409/5000\n",
      "1/1 [==============================] - 0s 260ms/step - loss: 0.1763 - val_loss: 162042.1562\n",
      "Epoch 410/5000\n",
      "1/1 [==============================] - 0s 265ms/step - loss: 0.1763 - val_loss: 162543.1250\n",
      "Epoch 411/5000\n",
      "1/1 [==============================] - 0s 264ms/step - loss: 0.1763 - val_loss: 163037.4844\n",
      "Epoch 412/5000\n",
      "1/1 [==============================] - 0s 262ms/step - loss: 0.1763 - val_loss: 163390.0156\n",
      "Epoch 413/5000\n",
      "1/1 [==============================] - 0s 263ms/step - loss: 0.1763 - val_loss: 163694.0156\n",
      "Epoch 414/5000\n",
      "1/1 [==============================] - 0s 262ms/step - loss: 0.1763 - val_loss: 164071.0781\n",
      "Epoch 415/5000\n",
      "1/1 [==============================] - 0s 260ms/step - loss: 0.1763 - val_loss: 164464.1094\n",
      "\n",
      "Epoch 00415: ReduceLROnPlateau reducing learning rate to 9.9999988758398e-14.\n",
      "Epoch 416/5000\n",
      "1/1 [==============================] - 0s 260ms/step - loss: 0.1763 - val_loss: 164758.5781\n",
      "Epoch 417/5000\n",
      "1/1 [==============================] - 0s 261ms/step - loss: 0.1763 - val_loss: 165045.7031\n",
      "Epoch 418/5000\n",
      "1/1 [==============================] - 0s 267ms/step - loss: 0.1763 - val_loss: 165287.5781\n",
      "Epoch 419/5000\n",
      "1/1 [==============================] - 0s 262ms/step - loss: 0.1763 - val_loss: 165545.7031\n",
      "Epoch 420/5000\n",
      "1/1 [==============================] - 0s 265ms/step - loss: 0.1763 - val_loss: 165761.3125\n",
      "Epoch 421/5000\n",
      "1/1 [==============================] - 0s 270ms/step - loss: 0.1763 - val_loss: 165994.5781\n",
      "Epoch 422/5000\n",
      "1/1 [==============================] - 0s 265ms/step - loss: 0.1763 - val_loss: 166204.1875\n",
      "Epoch 423/5000\n",
      "1/1 [==============================] - 0s 263ms/step - loss: 0.1763 - val_loss: 166388.4844\n",
      "Epoch 424/5000\n",
      "1/1 [==============================] - 0s 261ms/step - loss: 0.1763 - val_loss: 166601.2500\n",
      "Epoch 425/5000\n",
      "1/1 [==============================] - 0s 260ms/step - loss: 0.1763 - val_loss: 166813.4844\n",
      "\n",
      "Epoch 00425: ReduceLROnPlateau reducing learning rate to 9.999999146890344e-15.\n",
      "Epoch 426/5000\n",
      "1/1 [==============================] - 0s 261ms/step - loss: 0.1763 - val_loss: 167031.6719\n",
      "Epoch 427/5000\n",
      "1/1 [==============================] - 0s 265ms/step - loss: 0.1763 - val_loss: 167248.3281\n",
      "Epoch 428/5000\n",
      "1/1 [==============================] - 0s 259ms/step - loss: 0.1763 - val_loss: 167458.7812\n",
      "Epoch 429/5000\n",
      "1/1 [==============================] - 0s 260ms/step - loss: 0.1763 - val_loss: 167703.4688\n",
      "Epoch 430/5000\n",
      "1/1 [==============================] - 0s 259ms/step - loss: 0.1763 - val_loss: 168068.4844\n",
      "Epoch 431/5000\n",
      "1/1 [==============================] - 0s 261ms/step - loss: 0.1763 - val_loss: 168395.7969\n",
      "Epoch 432/5000\n",
      "1/1 [==============================] - 0s 261ms/step - loss: 0.1763 - val_loss: 168705.2344\n",
      "Epoch 433/5000\n",
      "1/1 [==============================] - 0s 262ms/step - loss: 0.1763 - val_loss: 169016.9531\n",
      "Epoch 434/5000\n",
      "1/1 [==============================] - 0s 265ms/step - loss: 0.1763 - val_loss: 169300.4844\n",
      "Epoch 435/5000\n",
      "1/1 [==============================] - 0s 260ms/step - loss: 0.1763 - val_loss: 169521.5312\n",
      "\n",
      "Epoch 00435: ReduceLROnPlateau reducing learning rate to 9.999998977483753e-16.\n",
      "Epoch 436/5000\n",
      "1/1 [==============================] - 0s 261ms/step - loss: 0.1763 - val_loss: 169681.8281\n",
      "Epoch 437/5000\n",
      "1/1 [==============================] - 0s 263ms/step - loss: 0.1763 - val_loss: 169835.4062\n",
      "Epoch 438/5000\n",
      "1/1 [==============================] - 0s 260ms/step - loss: 0.1763 - val_loss: 169969.8750\n",
      "Epoch 439/5000\n",
      "1/1 [==============================] - 0s 260ms/step - loss: 0.1763 - val_loss: 170045.3281\n",
      "Epoch 440/5000\n",
      "1/1 [==============================] - 0s 260ms/step - loss: 0.1763 - val_loss: 170103.0625\n",
      "Epoch 441/5000\n",
      "1/1 [==============================] - 0s 260ms/step - loss: 0.1763 - val_loss: 170165.1094\n",
      "Epoch 442/5000\n",
      "1/1 [==============================] - 0s 259ms/step - loss: 0.1763 - val_loss: 170259.8594\n",
      "Epoch 443/5000\n",
      "1/1 [==============================] - 0s 261ms/step - loss: 0.1763 - val_loss: 170353.1406\n",
      "Epoch 444/5000\n",
      "1/1 [==============================] - 0s 262ms/step - loss: 0.1763 - val_loss: 170455.2500\n",
      "Epoch 445/5000\n",
      "1/1 [==============================] - 0s 261ms/step - loss: 0.1763 - val_loss: 170525.4375\n",
      "\n",
      "Epoch 00445: ReduceLROnPlateau reducing learning rate to 9.999998977483754e-17.\n",
      "Epoch 446/5000\n",
      "1/1 [==============================] - 0s 260ms/step - loss: 0.1763 - val_loss: 170612.4375\n",
      "Epoch 447/5000\n",
      "1/1 [==============================] - 0s 259ms/step - loss: 0.1763 - val_loss: 170752.7031\n",
      "Epoch 448/5000\n",
      "1/1 [==============================] - 0s 260ms/step - loss: 0.1763 - val_loss: 170924.4375\n",
      "Epoch 449/5000\n",
      "1/1 [==============================] - 0s 262ms/step - loss: 0.1763 - val_loss: 171094.8906\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 450/5000\n",
      "1/1 [==============================] - 0s 263ms/step - loss: 0.1763 - val_loss: 171206.3438\n",
      "Epoch 451/5000\n",
      "1/1 [==============================] - 0s 271ms/step - loss: 0.1763 - val_loss: 171295.3750\n",
      "Epoch 452/5000\n",
      "1/1 [==============================] - 0s 261ms/step - loss: 0.1763 - val_loss: 171325.9844\n",
      "Epoch 453/5000\n",
      "1/1 [==============================] - 0s 267ms/step - loss: 0.1763 - val_loss: 171394.5000\n",
      "Epoch 454/5000\n",
      "1/1 [==============================] - 0s 265ms/step - loss: 0.1763 - val_loss: 171491.2188\n",
      "Epoch 455/5000\n",
      "1/1 [==============================] - 0s 262ms/step - loss: 0.1763 - val_loss: 171566.1562\n",
      "\n",
      "Epoch 00455: ReduceLROnPlateau reducing learning rate to 9.999998845134856e-18.\n",
      "Epoch 456/5000\n",
      "1/1 [==============================] - 0s 266ms/step - loss: 0.1763 - val_loss: 171665.2031\n",
      "Epoch 457/5000\n",
      "1/1 [==============================] - 0s 278ms/step - loss: 0.1763 - val_loss: 171799.5156\n",
      "Epoch 458/5000\n",
      "1/1 [==============================] - 0s 271ms/step - loss: 0.1763 - val_loss: 171942.2656\n",
      "Epoch 459/5000\n",
      "1/1 [==============================] - 0s 279ms/step - loss: 0.1763 - val_loss: 172067.7969\n",
      "Epoch 460/5000\n",
      "1/1 [==============================] - 0s 290ms/step - loss: 0.1763 - val_loss: 172157.8906\n",
      "Epoch 461/5000\n",
      "1/1 [==============================] - 0s 280ms/step - loss: 0.1763 - val_loss: 172241.0000\n",
      "Epoch 462/5000\n",
      "1/1 [==============================] - 0s 275ms/step - loss: 0.1763 - val_loss: 172316.9688\n",
      "Epoch 463/5000\n",
      "1/1 [==============================] - 0s 265ms/step - loss: 0.1763 - val_loss: 172400.6719\n",
      "Epoch 464/5000\n",
      "1/1 [==============================] - 0s 264ms/step - loss: 0.1763 - val_loss: 172490.5312\n",
      "Epoch 465/5000\n",
      "1/1 [==============================] - 0s 266ms/step - loss: 0.1763 - val_loss: 172676.2344\n",
      "\n",
      "Epoch 00465: ReduceLROnPlateau reducing learning rate to 9.999999010570977e-19.\n",
      "Epoch 466/5000\n",
      "1/1 [==============================] - 0s 260ms/step - loss: 0.1763 - val_loss: 172854.5938\n",
      "Epoch 467/5000\n",
      "1/1 [==============================] - 0s 260ms/step - loss: 0.1763 - val_loss: 173064.0781\n",
      "Epoch 468/5000\n",
      "1/1 [==============================] - 0s 258ms/step - loss: 0.1763 - val_loss: 173300.5000\n",
      "Epoch 469/5000\n",
      "1/1 [==============================] - 0s 260ms/step - loss: 0.1763 - val_loss: 173479.8750\n",
      "Epoch 470/5000\n",
      "1/1 [==============================] - 0s 260ms/step - loss: 0.1763 - val_loss: 173665.4375\n",
      "Epoch 471/5000\n",
      "1/1 [==============================] - 0s 261ms/step - loss: 0.1763 - val_loss: 173781.8750\n",
      "Epoch 472/5000\n",
      "1/1 [==============================] - 0s 267ms/step - loss: 0.1763 - val_loss: 173944.1719\n",
      "Epoch 473/5000\n",
      "1/1 [==============================] - 0s 261ms/step - loss: 0.1763 - val_loss: 174079.6406\n",
      "Epoch 474/5000\n",
      "1/1 [==============================] - 0s 260ms/step - loss: 0.1763 - val_loss: 174237.3594\n",
      "Epoch 475/5000\n",
      "1/1 [==============================] - 0s 261ms/step - loss: 0.1763 - val_loss: 174415.8438\n",
      "\n",
      "Epoch 00475: ReduceLROnPlateau reducing learning rate to 9.999999424161285e-20.\n",
      "Epoch 476/5000\n",
      "1/1 [==============================] - 0s 264ms/step - loss: 0.1763 - val_loss: 174610.2031\n",
      "Epoch 477/5000\n",
      "1/1 [==============================] - 0s 263ms/step - loss: 0.1763 - val_loss: 174760.8281\n",
      "Epoch 478/5000\n",
      "1/1 [==============================] - 0s 261ms/step - loss: 0.1763 - val_loss: 174894.9062\n",
      "Epoch 479/5000\n",
      "1/1 [==============================] - 0s 260ms/step - loss: 0.1763 - val_loss: 175030.6406\n",
      "Epoch 480/5000\n",
      "1/1 [==============================] - 0s 260ms/step - loss: 0.1763 - val_loss: 175185.7656\n",
      "Epoch 481/5000\n",
      "1/1 [==============================] - 0s 268ms/step - loss: 0.1763 - val_loss: 175373.2188\n",
      "Epoch 482/5000\n",
      "1/1 [==============================] - 0s 264ms/step - loss: 0.1763 - val_loss: 175527.8906\n",
      "Epoch 483/5000\n",
      "1/1 [==============================] - 0s 261ms/step - loss: 0.1763 - val_loss: 175680.7656\n",
      "Epoch 484/5000\n",
      "1/1 [==============================] - 0s 260ms/step - loss: 0.1763 - val_loss: 175882.0781\n",
      "Epoch 485/5000\n",
      "1/1 [==============================] - 0s 259ms/step - loss: 0.1763 - val_loss: 176068.0625\n",
      "\n",
      "Epoch 00485: ReduceLROnPlateau reducing learning rate to 9.999999682655225e-21.\n",
      "Epoch 486/5000\n",
      "1/1 [==============================] - 0s 262ms/step - loss: 0.1763 - val_loss: 176260.7812\n",
      "Epoch 487/5000\n",
      "1/1 [==============================] - 0s 262ms/step - loss: 0.1763 - val_loss: 176451.5781\n",
      "Epoch 488/5000\n",
      "1/1 [==============================] - 0s 262ms/step - loss: 0.1763 - val_loss: 176687.5469\n",
      "Epoch 489/5000\n",
      "1/1 [==============================] - 0s 261ms/step - loss: 0.1763 - val_loss: 176895.4219\n",
      "Epoch 490/5000\n",
      "1/1 [==============================] - 0s 262ms/step - loss: 0.1763 - val_loss: 177113.4219\n",
      "Epoch 491/5000\n",
      "1/1 [==============================] - 0s 261ms/step - loss: 0.1763 - val_loss: 177314.1094\n",
      "Epoch 492/5000\n",
      "1/1 [==============================] - 0s 259ms/step - loss: 0.1763 - val_loss: 177515.9375\n",
      "Epoch 493/5000\n",
      "1/1 [==============================] - 0s 261ms/step - loss: 0.1763 - val_loss: 177709.9219\n",
      "Epoch 494/5000\n",
      "1/1 [==============================] - 0s 262ms/step - loss: 0.1763 - val_loss: 177859.5000\n",
      "Epoch 495/5000\n",
      "1/1 [==============================] - 0s 266ms/step - loss: 0.1763 - val_loss: 178035.7656\n",
      "\n",
      "Epoch 00495: ReduceLROnPlateau reducing learning rate to 9.999999682655225e-22.\n",
      "Epoch 496/5000\n",
      "1/1 [==============================] - 0s 261ms/step - loss: 0.1763 - val_loss: 178167.5469\n",
      "Epoch 497/5000\n",
      "1/1 [==============================] - 0s 263ms/step - loss: 0.1763 - val_loss: 178285.6562\n",
      "Epoch 498/5000\n",
      "1/1 [==============================] - 0s 267ms/step - loss: 0.1763 - val_loss: 178403.6094\n",
      "Epoch 499/5000\n",
      "1/1 [==============================] - 0s 261ms/step - loss: 0.1763 - val_loss: 178501.7812\n",
      "Epoch 500/5000\n",
      "1/1 [==============================] - 0s 260ms/step - loss: 0.1763 - val_loss: 178624.9531\n",
      "Epoch 501/5000\n",
      "1/1 [==============================] - 0s 260ms/step - loss: 0.1763 - val_loss: 178727.5156\n",
      "Epoch 502/5000\n",
      "1/1 [==============================] - 0s 259ms/step - loss: 0.1763 - val_loss: 178836.4062\n",
      "Epoch 503/5000\n",
      "1/1 [==============================] - 0s 260ms/step - loss: 0.1763 - val_loss: 178938.0469\n",
      "Epoch 504/5000\n",
      "1/1 [==============================] - 0s 265ms/step - loss: 0.1763 - val_loss: 179044.4375\n",
      "Epoch 505/5000\n",
      "1/1 [==============================] - 0s 279ms/step - loss: 0.1763 - val_loss: 179129.8594\n",
      "\n",
      "Epoch 00505: ReduceLROnPlateau reducing learning rate to 9.999999682655225e-23.\n",
      "Epoch 506/5000\n",
      "1/1 [==============================] - 0s 270ms/step - loss: 0.1763 - val_loss: 179237.8906\n",
      "Epoch 507/5000\n",
      "1/1 [==============================] - 0s 265ms/step - loss: 0.1763 - val_loss: 179311.2344\n",
      "Epoch 508/5000\n",
      "1/1 [==============================] - 0s 264ms/step - loss: 0.1763 - val_loss: 179399.8906\n",
      "Epoch 509/5000\n",
      "1/1 [==============================] - 0s 262ms/step - loss: 0.1763 - val_loss: 179479.6094\n",
      "Epoch 510/5000\n",
      "1/1 [==============================] - 0s 261ms/step - loss: 0.1763 - val_loss: 179560.6406\n",
      "Epoch 511/5000\n",
      "1/1 [==============================] - 0s 268ms/step - loss: 0.1763 - val_loss: 179611.1562\n",
      "Epoch 512/5000\n",
      "1/1 [==============================] - 0s 261ms/step - loss: 0.1763 - val_loss: 179662.1406\n",
      "Epoch 513/5000\n",
      "1/1 [==============================] - 0s 261ms/step - loss: 0.1763 - val_loss: 179717.1250\n",
      "Epoch 514/5000\n",
      "1/1 [==============================] - 0s 264ms/step - loss: 0.1763 - val_loss: 179740.7344\n",
      "Epoch 515/5000\n",
      "1/1 [==============================] - 0s 264ms/step - loss: 0.1763 - val_loss: 179785.2344\n",
      "\n",
      "Epoch 00515: ReduceLROnPlateau reducing learning rate to 9.999999682655227e-24.\n",
      "Epoch 516/5000\n",
      "1/1 [==============================] - 0s 260ms/step - loss: 0.1763 - val_loss: 179801.5312\n",
      "Epoch 517/5000\n",
      "1/1 [==============================] - 0s 261ms/step - loss: 0.1763 - val_loss: 179842.1250\n",
      "Epoch 518/5000\n",
      "1/1 [==============================] - 0s 260ms/step - loss: 0.1763 - val_loss: 179859.5156\n",
      "Epoch 519/5000\n",
      "1/1 [==============================] - 0s 258ms/step - loss: 0.1763 - val_loss: 179908.8750\n",
      "Epoch 520/5000\n",
      "1/1 [==============================] - 0s 261ms/step - loss: 0.1763 - val_loss: 179959.2812\n",
      "Epoch 521/5000\n",
      "1/1 [==============================] - 0s 265ms/step - loss: 0.1763 - val_loss: 180000.7656\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 522/5000\n",
      "1/1 [==============================] - 0s 262ms/step - loss: 0.1763 - val_loss: 180024.4531\n",
      "Epoch 523/5000\n",
      "1/1 [==============================] - 0s 261ms/step - loss: 0.1763 - val_loss: 180063.5156\n",
      "Epoch 524/5000\n",
      "1/1 [==============================] - 0s 259ms/step - loss: 0.1763 - val_loss: 180088.0938\n",
      "Epoch 525/5000\n",
      "1/1 [==============================] - 0s 261ms/step - loss: 0.1763 - val_loss: 180113.2031\n",
      "\n",
      "Epoch 00525: ReduceLROnPlateau reducing learning rate to 9.999999998199588e-25.\n",
      "Epoch 526/5000\n",
      "1/1 [==============================] - 0s 266ms/step - loss: 0.1763 - val_loss: 180139.2500\n",
      "Epoch 527/5000\n",
      "1/1 [==============================] - 0s 263ms/step - loss: 0.1763 - val_loss: 180150.1250\n",
      "Epoch 528/5000\n",
      "1/1 [==============================] - 0s 261ms/step - loss: 0.1763 - val_loss: 180168.2188\n",
      "Epoch 529/5000\n",
      "1/1 [==============================] - 0s 260ms/step - loss: 0.1763 - val_loss: 180193.7969\n",
      "Epoch 530/5000\n",
      "1/1 [==============================] - 0s 261ms/step - loss: 0.1763 - val_loss: 180220.1406\n",
      "Epoch 531/5000\n",
      "1/1 [==============================] - 0s 259ms/step - loss: 0.1763 - val_loss: 180257.7656\n",
      "Epoch 532/5000\n",
      "1/1 [==============================] - 0s 262ms/step - loss: 0.1763 - val_loss: 180314.1562\n",
      "Epoch 533/5000\n",
      "1/1 [==============================] - 0s 261ms/step - loss: 0.1763 - val_loss: 180346.7188\n",
      "Epoch 534/5000\n",
      "1/1 [==============================] - 0s 260ms/step - loss: 0.1763 - val_loss: 180404.0000\n",
      "Epoch 535/5000\n",
      "1/1 [==============================] - 0s 263ms/step - loss: 0.1763 - val_loss: 180480.1250\n",
      "\n",
      "Epoch 00535: ReduceLROnPlateau reducing learning rate to 1.0000000195414814e-25.\n",
      "Epoch 536/5000\n",
      "1/1 [==============================] - 0s 262ms/step - loss: 0.1763 - val_loss: 180524.4375\n",
      "Epoch 537/5000\n",
      "1/1 [==============================] - 0s 265ms/step - loss: 0.1763 - val_loss: 180573.3125\n",
      "Epoch 538/5000\n",
      "1/1 [==============================] - 0s 260ms/step - loss: 0.1763 - val_loss: 180634.8750\n",
      "Epoch 539/5000\n",
      "1/1 [==============================] - 0s 259ms/step - loss: 0.1763 - val_loss: 180659.4219\n",
      "Epoch 540/5000\n",
      "1/1 [==============================] - 0s 259ms/step - loss: 0.1763 - val_loss: 180686.7188\n",
      "Epoch 541/5000\n",
      "1/1 [==============================] - 0s 260ms/step - loss: 0.1763 - val_loss: 180717.5781\n",
      "Epoch 542/5000\n",
      "1/1 [==============================] - 0s 261ms/step - loss: 0.1763 - val_loss: 180741.4844\n",
      "Epoch 543/5000\n",
      "1/1 [==============================] - 0s 259ms/step - loss: 0.1763 - val_loss: 180764.2188\n",
      "Epoch 544/5000\n",
      "1/1 [==============================] - 0s 260ms/step - loss: 0.1763 - val_loss: 180795.7969\n",
      "Epoch 545/5000\n",
      "1/1 [==============================] - 0s 260ms/step - loss: 0.1763 - val_loss: 180816.8906\n",
      "\n",
      "Epoch 00545: ReduceLROnPlateau reducing learning rate to 1.0000000195414814e-26.\n",
      "Epoch 546/5000\n",
      "1/1 [==============================] - 0s 261ms/step - loss: 0.1763 - val_loss: 180870.9219\n",
      "Epoch 547/5000\n",
      "1/1 [==============================] - 0s 263ms/step - loss: 0.1763 - val_loss: 180906.2969\n",
      "Epoch 548/5000\n",
      "1/1 [==============================] - 0s 269ms/step - loss: 0.1763 - val_loss: 180961.8594\n",
      "Epoch 549/5000\n",
      "1/1 [==============================] - 0s 260ms/step - loss: 0.1763 - val_loss: 181000.1719\n",
      "Epoch 550/5000\n",
      "1/1 [==============================] - 0s 261ms/step - loss: 0.1763 - val_loss: 181050.0781\n",
      "Epoch 551/5000\n",
      "1/1 [==============================] - 0s 264ms/step - loss: 0.1763 - val_loss: 181114.3438\n",
      "Epoch 552/5000\n",
      "1/1 [==============================] - 0s 261ms/step - loss: 0.1763 - val_loss: 181168.4219\n",
      "Epoch 553/5000\n",
      "1/1 [==============================] - 0s 263ms/step - loss: 0.1763 - val_loss: 181214.1719\n",
      "Epoch 554/5000\n",
      "1/1 [==============================] - 0s 271ms/step - loss: 0.1763 - val_loss: 181269.5156\n",
      "Epoch 555/5000\n",
      "1/1 [==============================] - 0s 263ms/step - loss: 0.1763 - val_loss: 181326.5312\n",
      "\n",
      "Epoch 00555: ReduceLROnPlateau reducing learning rate to 9.999999887266024e-28.\n",
      "Epoch 556/5000\n",
      "1/1 [==============================] - 0s 262ms/step - loss: 0.1763 - val_loss: 181384.6250\n",
      "Epoch 557/5000\n",
      "1/1 [==============================] - 0s 270ms/step - loss: 0.1763 - val_loss: 181458.6094\n",
      "Epoch 558/5000\n",
      "1/1 [==============================] - 0s 265ms/step - loss: 0.1763 - val_loss: 181503.1250\n",
      "Epoch 559/5000\n",
      "1/1 [==============================] - 0s 265ms/step - loss: 0.1763 - val_loss: 181551.7500\n",
      "Epoch 560/5000\n",
      "1/1 [==============================] - 0s 270ms/step - loss: 0.1763 - val_loss: 181612.1875\n",
      "Epoch 561/5000\n",
      "1/1 [==============================] - 0s 262ms/step - loss: 0.1763 - val_loss: 181653.0625\n",
      "Epoch 562/5000\n",
      "1/1 [==============================] - 0s 264ms/step - loss: 0.1763 - val_loss: 181703.2500\n",
      "Epoch 563/5000\n",
      "1/1 [==============================] - 0s 267ms/step - loss: 0.1763 - val_loss: 181776.3750\n",
      "Epoch 564/5000\n",
      "1/1 [==============================] - 0s 269ms/step - loss: 0.1763 - val_loss: 181837.8125\n",
      "Epoch 565/5000\n",
      "1/1 [==============================] - 0s 260ms/step - loss: 0.1763 - val_loss: 181892.8594\n",
      "\n",
      "Epoch 00565: ReduceLROnPlateau reducing learning rate to 1.0000000272452012e-28.\n",
      "Epoch 566/5000\n",
      "1/1 [==============================] - 0s 261ms/step - loss: 0.1763 - val_loss: 181950.9531\n",
      "Epoch 567/5000\n",
      "1/1 [==============================] - 0s 260ms/step - loss: 0.1763 - val_loss: 182004.8281\n",
      "Epoch 568/5000\n",
      "1/1 [==============================] - 0s 260ms/step - loss: 0.1763 - val_loss: 182069.0625\n",
      "Epoch 569/5000\n",
      "1/1 [==============================] - 0s 261ms/step - loss: 0.1763 - val_loss: 182116.0000\n",
      "Epoch 570/5000\n",
      "1/1 [==============================] - 0s 262ms/step - loss: 0.1763 - val_loss: 182174.0625\n",
      "Epoch 571/5000\n",
      "1/1 [==============================] - 0s 262ms/step - loss: 0.1763 - val_loss: 182224.2188\n",
      "Epoch 572/5000\n",
      "1/1 [==============================] - 0s 263ms/step - loss: 0.1763 - val_loss: 182288.5000\n",
      "Epoch 573/5000\n",
      "1/1 [==============================] - 0s 267ms/step - loss: 0.1763 - val_loss: 182370.1875\n",
      "Epoch 574/5000\n",
      "1/1 [==============================] - 0s 266ms/step - loss: 0.1763 - val_loss: 182423.6094\n",
      "Epoch 575/5000\n",
      "1/1 [==============================] - 0s 268ms/step - loss: 0.1763 - val_loss: 182497.1875\n",
      "\n",
      "Epoch 00575: ReduceLROnPlateau reducing learning rate to 1.0000000031710769e-29.\n",
      "Epoch 576/5000\n",
      "1/1 [==============================] - 0s 268ms/step - loss: 0.1763 - val_loss: 182561.9688\n",
      "Epoch 577/5000\n",
      "1/1 [==============================] - 0s 260ms/step - loss: 0.1763 - val_loss: 182657.8594\n",
      "Epoch 578/5000\n",
      "1/1 [==============================] - 0s 260ms/step - loss: 0.1763 - val_loss: 182728.2031\n",
      "Epoch 579/5000\n",
      "1/1 [==============================] - 0s 262ms/step - loss: 0.1763 - val_loss: 182815.0312\n",
      "Epoch 580/5000\n",
      "1/1 [==============================] - 0s 261ms/step - loss: 0.1763 - val_loss: 182901.2969\n",
      "Epoch 581/5000\n",
      "1/1 [==============================] - 0s 260ms/step - loss: 0.1763 - val_loss: 182998.7656\n",
      "Epoch 582/5000\n",
      "1/1 [==============================] - 0s 261ms/step - loss: 0.1763 - val_loss: 183060.5156\n",
      "Epoch 583/5000\n",
      "1/1 [==============================] - 0s 260ms/step - loss: 0.1763 - val_loss: 183153.0469\n",
      "Epoch 584/5000\n",
      "1/1 [==============================] - 0s 261ms/step - loss: 0.1763 - val_loss: 183242.0312\n",
      "Epoch 585/5000\n",
      "1/1 [==============================] - 0s 266ms/step - loss: 0.1763 - val_loss: 183327.8438\n",
      "\n",
      "Epoch 00585: ReduceLROnPlateau reducing learning rate to 1.0000000031710769e-30.\n",
      "Epoch 586/5000\n",
      "1/1 [==============================] - 0s 277ms/step - loss: 0.1763 - val_loss: 183398.1406\n",
      "Epoch 587/5000\n",
      "1/1 [==============================] - 0s 264ms/step - loss: 0.1763 - val_loss: 183460.4375\n",
      "Epoch 588/5000\n",
      "1/1 [==============================] - 0s 263ms/step - loss: 0.1763 - val_loss: 183540.0156\n",
      "Epoch 589/5000\n",
      "1/1 [==============================] - 0s 270ms/step - loss: 0.1763 - val_loss: 183612.8750\n",
      "Epoch 590/5000\n",
      "1/1 [==============================] - 0s 265ms/step - loss: 0.1763 - val_loss: 183694.4062\n",
      "Epoch 591/5000\n",
      "1/1 [==============================] - 0s 271ms/step - loss: 0.1763 - val_loss: 183771.7031\n",
      "Epoch 592/5000\n",
      "1/1 [==============================] - 0s 280ms/step - loss: 0.1763 - val_loss: 183853.5625\n",
      "Epoch 593/5000\n",
      "1/1 [==============================] - 0s 272ms/step - loss: 0.1763 - val_loss: 183946.3125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 594/5000\n",
      "1/1 [==============================] - 0s 270ms/step - loss: 0.1763 - val_loss: 184037.1406\n",
      "Epoch 595/5000\n",
      "1/1 [==============================] - 0s 271ms/step - loss: 0.1763 - val_loss: 184132.4688\n",
      "\n",
      "Epoch 00595: ReduceLROnPlateau reducing learning rate to 1.000000003171077e-31.\n",
      "Epoch 596/5000\n",
      "1/1 [==============================] - 0s 264ms/step - loss: 0.1763 - val_loss: 184196.7969\n",
      "Epoch 597/5000\n",
      "1/1 [==============================] - 0s 262ms/step - loss: 0.1763 - val_loss: 184298.1875\n",
      "Epoch 598/5000\n",
      "1/1 [==============================] - 0s 262ms/step - loss: 0.1763 - val_loss: 184395.9219\n",
      "Epoch 599/5000\n",
      "1/1 [==============================] - 0s 261ms/step - loss: 0.1763 - val_loss: 184489.4844\n",
      "Epoch 600/5000\n",
      "1/1 [==============================] - 0s 261ms/step - loss: 0.1763 - val_loss: 184576.3438\n",
      "Epoch 601/5000\n",
      "1/1 [==============================] - 0s 267ms/step - loss: 0.1763 - val_loss: 184647.8281\n",
      "Epoch 602/5000\n",
      "1/1 [==============================] - 0s 260ms/step - loss: 0.1763 - val_loss: 184742.8750\n",
      "Epoch 603/5000\n",
      "1/1 [==============================] - 0s 259ms/step - loss: 0.1763 - val_loss: 184822.7500\n",
      "Epoch 604/5000\n",
      "1/1 [==============================] - 0s 260ms/step - loss: 0.1763 - val_loss: 184908.2500\n",
      "Epoch 605/5000\n",
      "1/1 [==============================] - 0s 259ms/step - loss: 0.1763 - val_loss: 184984.8438\n",
      "\n",
      "Epoch 00605: ReduceLROnPlateau reducing learning rate to 9.999999796611899e-33.\n",
      "Epoch 606/5000\n",
      "1/1 [==============================] - 0s 260ms/step - loss: 0.1763 - val_loss: 185098.1250\n",
      "Epoch 607/5000\n",
      "1/1 [==============================] - 0s 260ms/step - loss: 0.1763 - val_loss: 185186.3750\n",
      "Epoch 608/5000\n",
      "1/1 [==============================] - 0s 261ms/step - loss: 0.1763 - val_loss: 185281.0781\n",
      "Epoch 609/5000\n",
      "1/1 [==============================] - 0s 260ms/step - loss: 0.1763 - val_loss: 185364.1875\n",
      "Epoch 610/5000\n",
      "1/1 [==============================] - 0s 266ms/step - loss: 0.1763 - val_loss: 185454.8594\n",
      "Epoch 611/5000\n",
      "1/1 [==============================] - 0s 277ms/step - loss: 0.1763 - val_loss: 185563.4844\n",
      "Epoch 612/5000\n",
      "1/1 [==============================] - 0s 268ms/step - loss: 0.1763 - val_loss: 185660.3594\n",
      "Epoch 613/5000\n",
      "1/1 [==============================] - 0s 261ms/step - loss: 0.1763 - val_loss: 185761.0000\n",
      "Epoch 614/5000\n",
      "1/1 [==============================] - 0s 261ms/step - loss: 0.1763 - val_loss: 185865.7031\n",
      "Epoch 615/5000\n",
      "1/1 [==============================] - 0s 260ms/step - loss: 0.1763 - val_loss: 185942.9531\n",
      "\n",
      "Epoch 00615: ReduceLROnPlateau reducing learning rate to 9.999999502738312e-34.\n",
      "Epoch 616/5000\n",
      "1/1 [==============================] - 0s 261ms/step - loss: 0.1763 - val_loss: 186022.1719\n",
      "Epoch 617/5000\n",
      "1/1 [==============================] - 0s 264ms/step - loss: 0.1763 - val_loss: 186116.5781\n",
      "Epoch 618/5000\n",
      "1/1 [==============================] - 0s 259ms/step - loss: 0.1763 - val_loss: 186201.7188\n",
      "Epoch 619/5000\n",
      "1/1 [==============================] - 0s 260ms/step - loss: 0.1763 - val_loss: 186314.4688\n",
      "Epoch 620/5000\n",
      "1/1 [==============================] - 0s 261ms/step - loss: 0.1763 - val_loss: 186412.6719\n",
      "Epoch 621/5000\n",
      "1/1 [==============================] - 0s 259ms/step - loss: 0.1763 - val_loss: 186506.0000\n",
      "Epoch 622/5000\n",
      "1/1 [==============================] - 0s 260ms/step - loss: 0.1763 - val_loss: 186619.5938\n",
      "Epoch 623/5000\n",
      "1/1 [==============================] - 0s 261ms/step - loss: 0.1763 - val_loss: 186717.7344\n",
      "Epoch 624/5000\n",
      "1/1 [==============================] - 0s 264ms/step - loss: 0.1763 - val_loss: 186807.7656\n",
      "Epoch 625/5000\n",
      "1/1 [==============================] - 0s 259ms/step - loss: 0.1763 - val_loss: 186908.2188\n",
      "\n",
      "Epoch 00625: ReduceLROnPlateau reducing learning rate to 9.999999319067318e-35.\n",
      "Epoch 626/5000\n",
      "1/1 [==============================] - 0s 261ms/step - loss: 0.1763 - val_loss: 186982.5781\n",
      "Epoch 627/5000\n",
      "1/1 [==============================] - 0s 263ms/step - loss: 0.1763 - val_loss: 187081.8594\n",
      "Epoch 628/5000\n",
      "1/1 [==============================] - 0s 260ms/step - loss: 0.1763 - val_loss: 187164.9688\n",
      "Epoch 629/5000\n",
      "1/1 [==============================] - 0s 260ms/step - loss: 0.1763 - val_loss: 187238.4844\n",
      "Epoch 630/5000\n",
      "1/1 [==============================] - 0s 259ms/step - loss: 0.1763 - val_loss: 187325.8750\n",
      "Epoch 631/5000\n",
      "1/1 [==============================] - 0s 263ms/step - loss: 0.1763 - val_loss: 187399.3750\n",
      "Epoch 632/5000\n",
      "1/1 [==============================] - 0s 260ms/step - loss: 0.1763 - val_loss: 187502.0781\n",
      "Epoch 633/5000\n",
      "1/1 [==============================] - 0s 261ms/step - loss: 0.1763 - val_loss: 187567.2188\n",
      "Epoch 634/5000\n",
      "1/1 [==============================] - 0s 263ms/step - loss: 0.1763 - val_loss: 187643.7500\n",
      "Epoch 635/5000\n",
      "1/1 [==============================] - 0s 262ms/step - loss: 0.1763 - val_loss: 187709.0781\n",
      "\n",
      "Epoch 00635: ReduceLROnPlateau reducing learning rate to 9.999999319067319e-36.\n",
      "Epoch 636/5000\n",
      "1/1 [==============================] - 0s 262ms/step - loss: 0.1763 - val_loss: 187783.8906\n",
      "Epoch 637/5000\n",
      "1/1 [==============================] - 0s 260ms/step - loss: 0.1763 - val_loss: 187849.2188\n",
      "Epoch 638/5000\n",
      "1/1 [==============================] - 0s 260ms/step - loss: 0.1763 - val_loss: 187921.7812\n",
      "Epoch 639/5000\n",
      "1/1 [==============================] - 0s 259ms/step - loss: 0.1763 - val_loss: 187984.9844\n",
      "Epoch 640/5000\n",
      "1/1 [==============================] - 0s 261ms/step - loss: 0.1763 - val_loss: 188062.0938\n",
      "Epoch 641/5000\n",
      "1/1 [==============================] - 0s 265ms/step - loss: 0.1763 - val_loss: 188123.4844\n",
      "Epoch 642/5000\n",
      "1/1 [==============================] - 0s 262ms/step - loss: 0.1763 - val_loss: 188195.7969\n",
      "Epoch 643/5000\n",
      "1/1 [==============================] - 0s 259ms/step - loss: 0.1763 - val_loss: 188253.6250\n",
      "Epoch 644/5000\n",
      "1/1 [==============================] - 0s 260ms/step - loss: 0.1763 - val_loss: 188333.7031\n",
      "Epoch 645/5000\n",
      "1/1 [==============================] - 0s 262ms/step - loss: 0.1763 - val_loss: 188387.1406\n",
      "\n",
      "Epoch 00645: ReduceLROnPlateau reducing learning rate to 9.999999462560281e-37.\n",
      "Epoch 646/5000\n",
      "1/1 [==============================] - 0s 259ms/step - loss: 0.1763 - val_loss: 188458.4688\n",
      "Epoch 647/5000\n",
      "1/1 [==============================] - 0s 259ms/step - loss: 0.1763 - val_loss: 188511.3906\n",
      "Epoch 648/5000\n",
      "1/1 [==============================] - 0s 260ms/step - loss: 0.1763 - val_loss: 188562.5469\n",
      "Epoch 649/5000\n",
      "1/1 [==============================] - 0s 260ms/step - loss: 0.1763 - val_loss: 188637.9375\n",
      "Epoch 650/5000\n",
      "1/1 [==============================] - 0s 262ms/step - loss: 0.1763 - val_loss: 188695.3750\n",
      "Epoch 651/5000\n",
      "1/1 [==============================] - 0s 267ms/step - loss: 0.1763 - val_loss: 188743.2188\n",
      "Epoch 652/5000\n",
      "1/1 [==============================] - 0s 262ms/step - loss: 0.1763 - val_loss: 188797.4375\n",
      "Epoch 653/5000\n",
      "1/1 [==============================] - 0s 263ms/step - loss: 0.1763 - val_loss: 188873.3125\n",
      "Epoch 654/5000\n",
      "1/1 [==============================] - 0s 262ms/step - loss: 0.1763 - val_loss: 188922.0625\n",
      "Epoch 655/5000\n",
      "1/1 [==============================] - 0s 260ms/step - loss: 0.1763 - val_loss: 188979.3125\n",
      "\n",
      "Epoch 00655: ReduceLROnPlateau reducing learning rate to 9.99999946256028e-38.\n",
      "Epoch 656/5000\n",
      "1/1 [==============================] - 0s 261ms/step - loss: 0.1763 - val_loss: 189041.1250\n",
      "Epoch 657/5000\n",
      "1/1 [==============================] - 0s 262ms/step - loss: 0.1763 - val_loss: 189092.4219\n",
      "Epoch 658/5000\n",
      "1/1 [==============================] - 0s 261ms/step - loss: 0.1763 - val_loss: 189153.7969\n",
      "Epoch 659/5000\n",
      "1/1 [==============================] - 0s 261ms/step - loss: 0.1763 - val_loss: 189210.8438\n",
      "Epoch 660/5000\n",
      "1/1 [==============================] - 0s 261ms/step - loss: 0.1763 - val_loss: 189251.8125\n",
      "Epoch 661/5000\n",
      "1/1 [==============================] - 0s 261ms/step - loss: 0.1763 - val_loss: 189314.9844\n",
      "Epoch 662/5000\n",
      "1/1 [==============================] - 0s 259ms/step - loss: 0.1763 - val_loss: 189363.1875\n",
      "Epoch 663/5000\n",
      "1/1 [==============================] - 0s 263ms/step - loss: 0.1763 - val_loss: 189408.0156\n",
      "Epoch 664/5000\n",
      "1/1 [==============================] - 0s 265ms/step - loss: 0.1763 - val_loss: 189450.7656\n",
      "Epoch 665/5000\n",
      "1/1 [==============================] - 0s 261ms/step - loss: 0.1763 - val_loss: 189519.5781\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00665: ReduceLROnPlateau reducing learning rate to 9.99999991097579e-39.\n",
      "Epoch 666/5000\n",
      "1/1 [==============================] - 0s 263ms/step - loss: 0.1763 - val_loss: 189562.3906\n",
      "Epoch 667/5000\n",
      "1/1 [==============================] - 0s 262ms/step - loss: 0.1763 - val_loss: 189612.5156\n",
      "Epoch 668/5000\n",
      "1/1 [==============================] - 0s 259ms/step - loss: 0.1763 - val_loss: 189658.0156\n",
      "Epoch 669/5000\n",
      "1/1 [==============================] - 0s 260ms/step - loss: 0.1763 - val_loss: 189695.6875\n",
      "Epoch 670/5000\n",
      "1/1 [==============================] - 0s 259ms/step - loss: 0.1763 - val_loss: 189754.3594\n",
      "Epoch 671/5000\n",
      "1/1 [==============================] - 0s 261ms/step - loss: 0.1763 - val_loss: 189802.5156\n",
      "Epoch 672/5000\n",
      "1/1 [==============================] - 0s 260ms/step - loss: 0.1763 - val_loss: 189835.3750\n",
      "Epoch 673/5000\n",
      "1/1 [==============================] - 0s 261ms/step - loss: 0.1763 - val_loss: 189894.4219\n",
      "Epoch 674/5000\n",
      "1/1 [==============================] - 0s 262ms/step - loss: 0.1763 - val_loss: 189935.6094\n",
      "Epoch 675/5000\n",
      "1/1 [==============================] - 0s 263ms/step - loss: 0.1763 - val_loss: 189987.2656\n",
      "\n",
      "Epoch 00675: ReduceLROnPlateau reducing learning rate to 9.999999350456405e-40.\n",
      "Epoch 676/5000\n",
      "1/1 [==============================] - 0s 260ms/step - loss: 0.1763 - val_loss: 190027.0000\n",
      "Epoch 677/5000\n",
      "1/1 [==============================] - 0s 262ms/step - loss: 0.1763 - val_loss: 190058.3906\n",
      "Epoch 678/5000\n",
      "1/1 [==============================] - 0s 261ms/step - loss: 0.1763 - val_loss: 190110.3750\n",
      "Epoch 679/5000\n",
      "1/1 [==============================] - 0s 261ms/step - loss: 0.1763 - val_loss: 190151.5781\n",
      "Epoch 680/5000\n",
      "1/1 [==============================] - 0s 260ms/step - loss: 0.1763 - val_loss: 190172.9062\n",
      "Epoch 681/5000\n",
      "1/1 [==============================] - 0s 266ms/step - loss: 0.1763 - val_loss: 190212.6875\n",
      "Epoch 682/5000\n",
      "1/1 [==============================] - 0s 259ms/step - loss: 0.1763 - val_loss: 190247.2656\n",
      "Epoch 683/5000\n",
      "1/1 [==============================] - 0s 260ms/step - loss: 0.1763 - val_loss: 190287.2656\n",
      "Epoch 684/5000\n",
      "1/1 [==============================] - 0s 262ms/step - loss: 0.1763 - val_loss: 190313.2656\n",
      "Epoch 685/5000\n",
      "1/1 [==============================] - 0s 259ms/step - loss: 0.1763 - val_loss: 190345.1719\n",
      "\n",
      "Epoch 00685: ReduceLROnPlateau reducing learning rate to 1.0000002153053334e-40.\n",
      "Epoch 686/5000\n",
      "1/1 [==============================] - 0s 260ms/step - loss: 0.1763 - val_loss: 190385.1562\n",
      "Epoch 687/5000\n",
      "1/1 [==============================] - 0s 259ms/step - loss: 0.1763 - val_loss: 190424.8438\n",
      "Epoch 688/5000\n",
      "1/1 [==============================] - 0s 261ms/step - loss: 0.1763 - val_loss: 190445.4531\n",
      "Epoch 689/5000\n",
      "1/1 [==============================] - 0s 259ms/step - loss: 0.1763 - val_loss: 190483.7188\n",
      "Epoch 690/5000\n",
      "1/1 [==============================] - 0s 263ms/step - loss: 0.1763 - val_loss: 190518.9375\n",
      "Epoch 691/5000\n",
      "1/1 [==============================] - 0s 266ms/step - loss: 0.1763 - val_loss: 190544.0781\n",
      "Epoch 692/5000\n",
      "1/1 [==============================] - 0s 262ms/step - loss: 0.1763 - val_loss: 190573.8281\n",
      "Epoch 693/5000\n",
      "1/1 [==============================] - 0s 260ms/step - loss: 0.1763 - val_loss: 190608.2969\n",
      "Epoch 694/5000\n",
      "1/1 [==============================] - 0s 261ms/step - loss: 0.1763 - val_loss: 190628.1094\n",
      "Epoch 695/5000\n",
      "1/1 [==============================] - 0s 261ms/step - loss: 0.1763 - val_loss: 190672.8125\n",
      "\n",
      "Epoch 00695: ReduceLROnPlateau reducing learning rate to 9.99994610111476e-42.\n",
      "Epoch 696/5000\n",
      "1/1 [==============================] - 0s 261ms/step - loss: 0.1763 - val_loss: 190686.5938\n",
      "Epoch 697/5000\n",
      "1/1 [==============================] - 0s 260ms/step - loss: 0.1763 - val_loss: 190718.6719\n",
      "Epoch 698/5000\n",
      "1/1 [==============================] - 0s 260ms/step - loss: 0.1763 - val_loss: 190757.9844\n",
      "Epoch 699/5000\n",
      "1/1 [==============================] - 0s 260ms/step - loss: 0.1763 - val_loss: 190782.1719\n",
      "Epoch 700/5000\n",
      "1/1 [==============================] - 0s 261ms/step - loss: 0.1763 - val_loss: 190809.5781\n",
      "Epoch 701/5000\n",
      "1/1 [==============================] - 0s 264ms/step - loss: 0.1763 - val_loss: 190832.3438\n",
      "Epoch 702/5000\n",
      "1/1 [==============================] - 0s 262ms/step - loss: 0.1763 - val_loss: 190867.6406\n",
      "Epoch 703/5000\n",
      "1/1 [==============================] - 0s 262ms/step - loss: 0.1763 - val_loss: 190874.3281\n",
      "Epoch 704/5000\n",
      "1/1 [==============================] - 0s 261ms/step - loss: 0.1763 - val_loss: 190914.3281\n",
      "Epoch 705/5000\n",
      "1/1 [==============================] - 0s 261ms/step - loss: 0.1763 - val_loss: 190940.8594\n",
      "\n",
      "Epoch 00705: ReduceLROnPlateau reducing learning rate to 9.999665841421895e-43.\n",
      "Epoch 706/5000\n",
      "1/1 [==============================] - 0s 259ms/step - loss: 0.1763 - val_loss: 190959.2500\n",
      "Epoch 707/5000\n",
      "1/1 [==============================] - 0s 260ms/step - loss: 0.1763 - val_loss: 190982.3438\n",
      "Epoch 708/5000\n",
      "1/1 [==============================] - 0s 260ms/step - loss: 0.1763 - val_loss: 191020.6562\n",
      "Epoch 709/5000\n",
      "1/1 [==============================] - 0s 261ms/step - loss: 0.1763 - val_loss: 191047.5625\n",
      "Epoch 710/5000\n",
      "1/1 [==============================] - 0s 261ms/step - loss: 0.1763 - val_loss: 191062.6562\n",
      "Epoch 711/5000\n",
      "1/1 [==============================] - 0s 266ms/step - loss: 0.1763 - val_loss: 191089.9688\n",
      "Epoch 712/5000\n",
      "1/1 [==============================] - 0s 261ms/step - loss: 0.1763 - val_loss: 191113.9531\n",
      "Epoch 713/5000\n",
      "1/1 [==============================] - 0s 263ms/step - loss: 0.1763 - val_loss: 191120.7656\n",
      "Epoch 714/5000\n",
      "1/1 [==============================] - 0s 261ms/step - loss: 0.1763 - val_loss: 191147.1406\n",
      "Epoch 715/5000\n",
      "1/1 [==============================] - 0s 261ms/step - loss: 0.1763 - val_loss: 191179.8906\n",
      "\n",
      "Epoch 00715: ReduceLROnPlateau reducing learning rate to 1.0005271035279195e-43.\n",
      "Epoch 716/5000\n",
      "1/1 [==============================] - 0s 262ms/step - loss: 0.1763 - val_loss: 191222.4531\n",
      "Epoch 717/5000\n",
      "1/1 [==============================] - 0s 263ms/step - loss: 0.1763 - val_loss: 191225.5781\n",
      "Epoch 718/5000\n",
      "1/1 [==============================] - 0s 260ms/step - loss: 0.1763 - val_loss: 191250.2188\n",
      "Epoch 719/5000\n",
      "1/1 [==============================] - 0s 261ms/step - loss: 0.1763 - val_loss: 191282.9375\n",
      "Epoch 720/5000\n",
      "1/1 [==============================] - 0s 260ms/step - loss: 0.1763 - val_loss: 191298.9688\n",
      "Epoch 721/5000\n",
      "1/1 [==============================] - 0s 262ms/step - loss: 0.1763 - val_loss: 191307.9219\n",
      "Epoch 722/5000\n",
      "1/1 [==============================] - 0s 260ms/step - loss: 0.1763 - val_loss: 191339.5781\n",
      "Epoch 723/5000\n",
      "1/1 [==============================] - 0s 261ms/step - loss: 0.1763 - val_loss: 191359.4531\n",
      "Epoch 724/5000\n",
      "1/1 [==============================] - 0s 264ms/step - loss: 0.1763 - val_loss: 191377.7812\n",
      "Epoch 725/5000\n",
      "1/1 [==============================] - 0s 267ms/step - loss: 0.1763 - val_loss: 191392.2500\n",
      "\n",
      "Epoch 00725: ReduceLROnPlateau reducing learning rate to 9.949219096706202e-45.\n",
      "Epoch 726/5000\n",
      "1/1 [==============================] - 0s 266ms/step - loss: 0.1763 - val_loss: 191422.4688\n",
      "Epoch 727/5000\n",
      "1/1 [==============================] - 0s 267ms/step - loss: 0.1763 - val_loss: 191439.1250\n",
      "Epoch 728/5000\n",
      "1/1 [==============================] - 0s 280ms/step - loss: 0.1763 - val_loss: 191469.6094\n",
      "Epoch 729/5000\n",
      "1/1 [==============================] - 0s 275ms/step - loss: 0.1763 - val_loss: 191482.8281\n",
      "Epoch 730/5000\n",
      "1/1 [==============================] - 0s 291ms/step - loss: 0.1763 - val_loss: 191511.4375\n",
      "Epoch 731/5000\n",
      "1/1 [==============================] - 0s 298ms/step - loss: 0.1763 - val_loss: 191536.9844\n",
      "Epoch 732/5000\n",
      "1/1 [==============================] - 0s 309ms/step - loss: 0.1763 - val_loss: 191554.5781\n",
      "Epoch 733/5000\n",
      "1/1 [==============================] - 0s 285ms/step - loss: 0.1763 - val_loss: 191574.8125\n",
      "Epoch 734/5000\n",
      "1/1 [==============================] - 0s 271ms/step - loss: 0.1763 - val_loss: 191595.0781\n",
      "Epoch 735/5000\n",
      "1/1 [==============================] - 0s 280ms/step - loss: 0.1763 - val_loss: 191613.6250\n",
      "\n",
      "Epoch 00735: ReduceLROnPlateau reducing learning rate to 9.80908925027372e-46.\n",
      "Epoch 736/5000\n",
      "1/1 [==============================] - 0s 278ms/step - loss: 0.1763 - val_loss: 191643.6250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 737/5000\n",
      "1/1 [==============================] - 0s 271ms/step - loss: 0.1763 - val_loss: 191646.6719\n",
      "Epoch 738/5000\n",
      "1/1 [==============================] - 0s 269ms/step - loss: 0.1763 - val_loss: 191680.7656\n",
      "Epoch 739/5000\n",
      "1/1 [==============================] - 0s 269ms/step - loss: 0.1763 - val_loss: 191692.0469\n",
      "Epoch 740/5000\n",
      "1/1 [==============================] - 0s 261ms/step - loss: 0.1763 - val_loss: 191704.3125\n",
      "Epoch 741/5000\n",
      "1/1 [==============================] - 0s 262ms/step - loss: 0.1763 - val_loss: 191729.0938\n",
      "Epoch 742/5000\n",
      "1/1 [==============================] - 0s 260ms/step - loss: 0.1763 - val_loss: 191751.2031\n",
      "Epoch 743/5000\n",
      "1/1 [==============================] - 0s 265ms/step - loss: 0.1763 - val_loss: 191754.0625\n",
      "Epoch 744/5000\n",
      "1/1 [==============================] - 0s 262ms/step - loss: 0.1763 - val_loss: 191773.6250\n",
      "Epoch 745/5000\n",
      "1/1 [==============================] - 0s 264ms/step - loss: 0.1763 - val_loss: 191789.5938\n",
      "\n",
      "Epoch 00745: ReduceLROnPlateau reducing learning rate to 1.4012984643248171e-46.\n",
      "Epoch 746/5000\n",
      "1/1 [==============================] - 0s 266ms/step - loss: 0.1763 - val_loss: 191809.1250\n",
      "Epoch 747/5000\n",
      "1/1 [==============================] - 0s 267ms/step - loss: 0.1763 - val_loss: 191830.3438\n",
      "Epoch 748/5000\n",
      "1/1 [==============================] - 0s 269ms/step - loss: 0.1763 - val_loss: 191846.3594\n",
      "Epoch 749/5000\n",
      "1/1 [==============================] - 0s 276ms/step - loss: 0.1763 - val_loss: 191860.5156\n",
      "Epoch 750/5000\n",
      "1/1 [==============================] - 0s 270ms/step - loss: 0.1763 - val_loss: 191868.9688\n",
      "Epoch 751/5000\n",
      "1/1 [==============================] - 0s 274ms/step - loss: 0.1763 - val_loss: 191891.9375\n",
      "Epoch 752/5000\n",
      "1/1 [==============================] - 0s 291ms/step - loss: 0.1763 - val_loss: 191907.8750\n",
      "Epoch 753/5000\n",
      "1/1 [==============================] - 0s 288ms/step - loss: 0.1763 - val_loss: 191913.0781\n",
      "Epoch 754/5000\n",
      "1/1 [==============================] - 0s 292ms/step - loss: 0.1763 - val_loss: 191937.4531\n",
      "Epoch 755/5000\n",
      "1/1 [==============================] - 0s 294ms/step - loss: 0.1763 - val_loss: 191956.9219\n",
      "Epoch 756/5000\n",
      "1/1 [==============================] - 0s 280ms/step - loss: 0.1763 - val_loss: 191960.5000\n",
      "Epoch 757/5000\n",
      "1/1 [==============================] - 0s 277ms/step - loss: 0.1763 - val_loss: 191977.6875\n",
      "Epoch 758/5000\n",
      "1/1 [==============================] - 0s 273ms/step - loss: 0.1763 - val_loss: 191990.1875\n",
      "Epoch 759/5000\n",
      "1/1 [==============================] - 0s 267ms/step - loss: 0.1763 - val_loss: 191998.5156\n",
      "Epoch 760/5000\n",
      "1/1 [==============================] - 0s 277ms/step - loss: 0.1763 - val_loss: 192011.4062\n",
      "Epoch 761/5000\n",
      "1/1 [==============================] - 0s 282ms/step - loss: 0.1763 - val_loss: 192039.2344\n",
      "Epoch 762/5000\n",
      "1/1 [==============================] - 0s 262ms/step - loss: 0.1763 - val_loss: 192045.0000\n",
      "Epoch 763/5000\n",
      "1/1 [==============================] - 0s 272ms/step - loss: 0.1763 - val_loss: 192055.5625\n",
      "Epoch 764/5000\n",
      "1/1 [==============================] - 0s 273ms/step - loss: 0.1763 - val_loss: 192071.9375\n",
      "Epoch 765/5000\n",
      "1/1 [==============================] - 0s 274ms/step - loss: 0.1763 - val_loss: 192083.2031\n",
      "Epoch 766/5000\n",
      "1/1 [==============================] - 0s 276ms/step - loss: 0.1763 - val_loss: 192116.5469\n",
      "Epoch 767/5000\n",
      "1/1 [==============================] - 0s 267ms/step - loss: 0.1763 - val_loss: 192129.1719\n",
      "Epoch 768/5000\n",
      "1/1 [==============================] - 0s 266ms/step - loss: 0.1763 - val_loss: 192122.5156\n",
      "Epoch 769/5000\n",
      "1/1 [==============================] - 0s 265ms/step - loss: 0.1763 - val_loss: 192141.3125\n",
      "Epoch 770/5000\n",
      "1/1 [==============================] - 0s 268ms/step - loss: 0.1763 - val_loss: 192146.6406\n",
      "Epoch 771/5000\n",
      "1/1 [==============================] - 0s 316ms/step - loss: 0.1763 - val_loss: 192171.9688\n",
      "Epoch 772/5000\n",
      "1/1 [==============================] - 0s 296ms/step - loss: 0.1763 - val_loss: 192185.3125\n",
      "Epoch 773/5000\n",
      "1/1 [==============================] - 0s 288ms/step - loss: 0.1763 - val_loss: 192199.2031\n",
      "Epoch 774/5000\n",
      "1/1 [==============================] - 0s 289ms/step - loss: 0.1763 - val_loss: 192201.7031\n",
      "Epoch 775/5000\n",
      "1/1 [==============================] - 0s 289ms/step - loss: 0.1763 - val_loss: 192219.7344\n",
      "Epoch 776/5000\n",
      "1/1 [==============================] - 0s 280ms/step - loss: 0.1763 - val_loss: 192231.7344\n",
      "Epoch 777/5000\n",
      "1/1 [==============================] - 0s 270ms/step - loss: 0.1763 - val_loss: 192250.4688\n",
      "Epoch 778/5000\n",
      "1/1 [==============================] - 0s 271ms/step - loss: 0.1763 - val_loss: 192242.7969\n",
      "Epoch 779/5000\n",
      "1/1 [==============================] - 0s 268ms/step - loss: 0.1763 - val_loss: 192261.2031\n",
      "Epoch 780/5000\n",
      "1/1 [==============================] - 0s 267ms/step - loss: 0.1763 - val_loss: 192277.1406\n",
      "Epoch 781/5000\n",
      "1/1 [==============================] - 0s 262ms/step - loss: 0.1763 - val_loss: 192298.7500\n",
      "Epoch 782/5000\n",
      "1/1 [==============================] - 0s 261ms/step - loss: 0.1763 - val_loss: 192304.2500\n",
      "Epoch 783/5000\n",
      "1/1 [==============================] - 0s 263ms/step - loss: 0.1763 - val_loss: 192319.3594\n",
      "Epoch 784/5000\n",
      "1/1 [==============================] - 0s 264ms/step - loss: 0.1763 - val_loss: 192325.0625\n",
      "Epoch 785/5000\n",
      "1/1 [==============================] - 0s 267ms/step - loss: 0.1763 - val_loss: 192349.6250\n",
      "Epoch 786/5000\n",
      "1/1 [==============================] - 0s 267ms/step - loss: 0.1763 - val_loss: 192365.8750\n",
      "Epoch 787/5000\n",
      "1/1 [==============================] - 0s 271ms/step - loss: 0.1763 - val_loss: 192372.8906\n",
      "Epoch 788/5000\n",
      "1/1 [==============================] - 0s 279ms/step - loss: 0.1763 - val_loss: 192386.3906\n",
      "Epoch 789/5000\n",
      "1/1 [==============================] - 0s 271ms/step - loss: 0.1763 - val_loss: 192396.0000\n",
      "Epoch 790/5000\n",
      "1/1 [==============================] - 0s 278ms/step - loss: 0.1763 - val_loss: 192406.5156\n",
      "Epoch 791/5000\n",
      "1/1 [==============================] - 0s 275ms/step - loss: 0.1763 - val_loss: 192410.1875\n",
      "Epoch 792/5000\n",
      "1/1 [==============================] - 0s 272ms/step - loss: 0.1763 - val_loss: 192435.5469\n",
      "Epoch 793/5000\n",
      "1/1 [==============================] - 0s 276ms/step - loss: 0.1763 - val_loss: 192446.1094\n",
      "Epoch 794/5000\n",
      "1/1 [==============================] - 0s 285ms/step - loss: 0.1763 - val_loss: 192451.9688\n",
      "Epoch 795/5000\n",
      "1/1 [==============================] - 0s 284ms/step - loss: 0.1763 - val_loss: 192462.5312\n",
      "Epoch 796/5000\n",
      "1/1 [==============================] - 0s 274ms/step - loss: 0.1763 - val_loss: 192480.3594\n",
      "Epoch 797/5000\n",
      "1/1 [==============================] - 0s 264ms/step - loss: 0.1763 - val_loss: 192488.0938\n",
      "Epoch 798/5000\n",
      "1/1 [==============================] - 0s 262ms/step - loss: 0.1763 - val_loss: 192509.9375\n",
      "Epoch 799/5000\n",
      "1/1 [==============================] - 0s 262ms/step - loss: 0.1763 - val_loss: 192519.8125\n",
      "Epoch 800/5000\n",
      "1/1 [==============================] - 0s 263ms/step - loss: 0.1763 - val_loss: 192522.2969\n",
      "Epoch 801/5000\n",
      "1/1 [==============================] - 0s 263ms/step - loss: 0.1763 - val_loss: 192530.9219\n",
      "Epoch 802/5000\n",
      "1/1 [==============================] - 0s 276ms/step - loss: 0.1763 - val_loss: 192540.2656\n",
      "Epoch 803/5000\n",
      "1/1 [==============================] - 0s 266ms/step - loss: 0.1763 - val_loss: 192550.4062\n",
      "Epoch 804/5000\n",
      "1/1 [==============================] - 0s 266ms/step - loss: 0.1763 - val_loss: 192571.3906\n",
      "Epoch 805/5000\n",
      "1/1 [==============================] - 0s 266ms/step - loss: 0.1763 - val_loss: 192587.6562\n",
      "Epoch 806/5000\n",
      "1/1 [==============================] - 0s 264ms/step - loss: 0.1763 - val_loss: 192604.7656\n",
      "Epoch 807/5000\n",
      "1/1 [==============================] - 0s 261ms/step - loss: 0.1763 - val_loss: 192595.0156\n",
      "Epoch 808/5000\n",
      "1/1 [==============================] - 0s 263ms/step - loss: 0.1763 - val_loss: 192616.3906\n",
      "Epoch 809/5000\n",
      "1/1 [==============================] - 0s 269ms/step - loss: 0.1763 - val_loss: 192613.8594\n",
      "Epoch 810/5000\n",
      "1/1 [==============================] - 0s 274ms/step - loss: 0.1763 - val_loss: 192627.2188\n",
      "Epoch 811/5000\n",
      "1/1 [==============================] - 0s 269ms/step - loss: 0.1763 - val_loss: 192644.7812\n",
      "Epoch 812/5000\n",
      "1/1 [==============================] - 0s 271ms/step - loss: 0.1763 - val_loss: 192649.4062\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 813/5000\n",
      "1/1 [==============================] - 0s 274ms/step - loss: 0.1763 - val_loss: 192666.7969\n",
      "Epoch 814/5000\n",
      "1/1 [==============================] - 0s 269ms/step - loss: 0.1763 - val_loss: 192681.9688\n",
      "Epoch 815/5000\n",
      "1/1 [==============================] - 0s 274ms/step - loss: 0.1763 - val_loss: 192671.3906\n",
      "Epoch 816/5000\n",
      "1/1 [==============================] - 0s 267ms/step - loss: 0.1763 - val_loss: 192691.2656\n",
      "Epoch 817/5000\n",
      "1/1 [==============================] - 0s 267ms/step - loss: 0.1763 - val_loss: 192699.9844\n",
      "Epoch 818/5000\n",
      "1/1 [==============================] - 0s 267ms/step - loss: 0.1763 - val_loss: 192712.2812\n",
      "Epoch 819/5000\n",
      "1/1 [==============================] - 0s 269ms/step - loss: 0.1763 - val_loss: 192726.9531\n",
      "Epoch 820/5000\n",
      "1/1 [==============================] - 0s 276ms/step - loss: 0.1763 - val_loss: 192741.4062\n",
      "Epoch 821/5000\n",
      "1/1 [==============================] - 0s 269ms/step - loss: 0.1763 - val_loss: 192744.9531\n",
      "Epoch 822/5000\n",
      "1/1 [==============================] - 0s 281ms/step - loss: 0.1763 - val_loss: 192752.2500\n",
      "Epoch 823/5000\n",
      "1/1 [==============================] - 0s 291ms/step - loss: 0.1763 - val_loss: 192754.4219\n",
      "Epoch 824/5000\n",
      "1/1 [==============================] - 0s 283ms/step - loss: 0.1763 - val_loss: 192773.5000\n",
      "Epoch 825/5000\n",
      "1/1 [==============================] - 0s 285ms/step - loss: 0.1763 - val_loss: 192778.2188\n",
      "Epoch 826/5000\n",
      "1/1 [==============================] - 0s 274ms/step - loss: 0.1763 - val_loss: 192791.5469\n",
      "Epoch 827/5000\n",
      "1/1 [==============================] - 0s 278ms/step - loss: 0.1763 - val_loss: 192799.1250\n",
      "Epoch 828/5000\n",
      "1/1 [==============================] - 0s 267ms/step - loss: 0.1763 - val_loss: 192805.7656\n",
      "Epoch 829/5000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-52-be869be8a603>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m           \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m           \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_image\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnewaxis\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m...\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minput_image\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnewaxis\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m...\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m          callbacks = [plateau])\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1703\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1704\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1705\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1706\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1707\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m   1233\u001b[0m                         \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1234\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1235\u001b[0;31m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1236\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1237\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2476\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2477\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[0;32m-> 2478\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2479\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    887\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 889\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    890\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1118\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1120\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1121\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1315\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1317\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1318\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1319\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1321\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1322\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1323\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1324\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1325\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1300\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1301\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1302\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "K.set_value(model.optimizer.lr, 1e-2)\n",
    "model.fit(input_image[np.newaxis,...],input_image[np.newaxis,...],\n",
    "          epochs=5000,\n",
    "          validation_data=(input_image[np.newaxis,...],input_image[np.newaxis,...]),\n",
    "         callbacks = [plateau])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "1/1 [==============================] - 0s 68ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1395956.25"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(input_image[np.newaxis,...],input_image[np.newaxis,...])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f560d355940>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "pred = model.predict(input_image[None,...])\n",
    "pred_norm = pred[0]- np.min(pred[0])\n",
    "pred_norm /= np.max(pred_norm)\n",
    "pred_norm *=255\n",
    "\n",
    "f, (ax1, ax2) = plt.subplots(1, 2)\n",
    "ax1.imshow(image.astype(np.uint8))\n",
    "ax2.imshow(pred_norm.astype(np.uint8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(pred.flatten())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([0.9794649, 0.9796191, 0.9711557], dtype=float32),\n",
       " array([ 0.03111083,  0.02761376, -0.02727177], dtype=float32),\n",
       " array([-0.98126704,  0.02382847, -0.12327314], dtype=float32),\n",
       " array([12.116167, 18.024914,  8.345219], dtype=float32)]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_layer('upsample_bn_conv1').get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_image = np.random.randn(260,260,3)\n",
    "# init_op = tf.global_variables_initializer()\n",
    "# with tf.Session() as sess:\n",
    "#     sess.run(init_op)\n",
    "#     res = sess.run(x, feed_dict={inputs: [input_image]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'bn_conv1/gamma:0' shape=(64,) dtype=float32_ref>,\n",
       " <tf.Variable 'bn_conv1/beta:0' shape=(64,) dtype=float32_ref>,\n",
       " <tf.Variable 'bn2a_branch2a/gamma:0' shape=(64,) dtype=float32_ref>,\n",
       " <tf.Variable 'bn2a_branch2a/beta:0' shape=(64,) dtype=float32_ref>,\n",
       " <tf.Variable 'bn2a_branch2b/gamma:0' shape=(64,) dtype=float32_ref>,\n",
       " <tf.Variable 'bn2a_branch2b/beta:0' shape=(64,) dtype=float32_ref>,\n",
       " <tf.Variable 'bn2a_branch2c/gamma:0' shape=(256,) dtype=float32_ref>,\n",
       " <tf.Variable 'bn2a_branch2c/beta:0' shape=(256,) dtype=float32_ref>,\n",
       " <tf.Variable 'bn2a_branch1/gamma:0' shape=(256,) dtype=float32_ref>,\n",
       " <tf.Variable 'bn2a_branch1/beta:0' shape=(256,) dtype=float32_ref>,\n",
       " <tf.Variable 'bn2b_branch2a/gamma:0' shape=(64,) dtype=float32_ref>,\n",
       " <tf.Variable 'bn2b_branch2a/beta:0' shape=(64,) dtype=float32_ref>,\n",
       " <tf.Variable 'bn2b_branch2b/gamma:0' shape=(64,) dtype=float32_ref>,\n",
       " <tf.Variable 'bn2b_branch2b/beta:0' shape=(64,) dtype=float32_ref>,\n",
       " <tf.Variable 'bn2b_branch2c/gamma:0' shape=(256,) dtype=float32_ref>,\n",
       " <tf.Variable 'bn2b_branch2c/beta:0' shape=(256,) dtype=float32_ref>,\n",
       " <tf.Variable 'bn2c_branch2a/gamma:0' shape=(64,) dtype=float32_ref>,\n",
       " <tf.Variable 'bn2c_branch2a/beta:0' shape=(64,) dtype=float32_ref>,\n",
       " <tf.Variable 'bn2c_branch2b/gamma:0' shape=(64,) dtype=float32_ref>,\n",
       " <tf.Variable 'bn2c_branch2b/beta:0' shape=(64,) dtype=float32_ref>,\n",
       " <tf.Variable 'bn2c_branch2c/gamma:0' shape=(256,) dtype=float32_ref>,\n",
       " <tf.Variable 'bn2c_branch2c/beta:0' shape=(256,) dtype=float32_ref>,\n",
       " <tf.Variable 'bn3a_branch2a/gamma:0' shape=(128,) dtype=float32_ref>,\n",
       " <tf.Variable 'bn3a_branch2a/beta:0' shape=(128,) dtype=float32_ref>,\n",
       " <tf.Variable 'bn3a_branch2b/gamma:0' shape=(128,) dtype=float32_ref>,\n",
       " <tf.Variable 'bn3a_branch2b/beta:0' shape=(128,) dtype=float32_ref>,\n",
       " <tf.Variable 'bn3a_branch2c/gamma:0' shape=(512,) dtype=float32_ref>,\n",
       " <tf.Variable 'bn3a_branch2c/beta:0' shape=(512,) dtype=float32_ref>,\n",
       " <tf.Variable 'bn3a_branch1/gamma:0' shape=(512,) dtype=float32_ref>,\n",
       " <tf.Variable 'bn3a_branch1/beta:0' shape=(512,) dtype=float32_ref>,\n",
       " <tf.Variable 'bn3b_branch2a/gamma:0' shape=(128,) dtype=float32_ref>,\n",
       " <tf.Variable 'bn3b_branch2a/beta:0' shape=(128,) dtype=float32_ref>,\n",
       " <tf.Variable 'bn3b_branch2b/gamma:0' shape=(128,) dtype=float32_ref>,\n",
       " <tf.Variable 'bn3b_branch2b/beta:0' shape=(128,) dtype=float32_ref>,\n",
       " <tf.Variable 'bn3b_branch2c/gamma:0' shape=(512,) dtype=float32_ref>,\n",
       " <tf.Variable 'bn3b_branch2c/beta:0' shape=(512,) dtype=float32_ref>,\n",
       " <tf.Variable 'bn3c_branch2a/gamma:0' shape=(128,) dtype=float32_ref>,\n",
       " <tf.Variable 'bn3c_branch2a/beta:0' shape=(128,) dtype=float32_ref>,\n",
       " <tf.Variable 'bn3c_branch2b/gamma:0' shape=(128,) dtype=float32_ref>,\n",
       " <tf.Variable 'bn3c_branch2b/beta:0' shape=(128,) dtype=float32_ref>,\n",
       " <tf.Variable 'bn3c_branch2c/gamma:0' shape=(512,) dtype=float32_ref>,\n",
       " <tf.Variable 'bn3c_branch2c/beta:0' shape=(512,) dtype=float32_ref>,\n",
       " <tf.Variable 'bn3d_branch2a/gamma:0' shape=(128,) dtype=float32_ref>,\n",
       " <tf.Variable 'bn3d_branch2a/beta:0' shape=(128,) dtype=float32_ref>,\n",
       " <tf.Variable 'bn3d_branch2b/gamma:0' shape=(128,) dtype=float32_ref>,\n",
       " <tf.Variable 'bn3d_branch2b/beta:0' shape=(128,) dtype=float32_ref>,\n",
       " <tf.Variable 'bn3d_branch2c/gamma:0' shape=(512,) dtype=float32_ref>,\n",
       " <tf.Variable 'bn3d_branch2c/beta:0' shape=(512,) dtype=float32_ref>,\n",
       " <tf.Variable 'bn4a_branch2a/gamma:0' shape=(256,) dtype=float32_ref>,\n",
       " <tf.Variable 'bn4a_branch2a/beta:0' shape=(256,) dtype=float32_ref>,\n",
       " <tf.Variable 'bn4a_branch2b/gamma:0' shape=(256,) dtype=float32_ref>,\n",
       " <tf.Variable 'bn4a_branch2b/beta:0' shape=(256,) dtype=float32_ref>,\n",
       " <tf.Variable 'bn4a_branch2c/gamma:0' shape=(1024,) dtype=float32_ref>,\n",
       " <tf.Variable 'bn4a_branch2c/beta:0' shape=(1024,) dtype=float32_ref>,\n",
       " <tf.Variable 'bn4a_branch1/gamma:0' shape=(1024,) dtype=float32_ref>,\n",
       " <tf.Variable 'bn4a_branch1/beta:0' shape=(1024,) dtype=float32_ref>,\n",
       " <tf.Variable 'bn4b_branch2a/gamma:0' shape=(256,) dtype=float32_ref>,\n",
       " <tf.Variable 'bn4b_branch2a/beta:0' shape=(256,) dtype=float32_ref>,\n",
       " <tf.Variable 'bn4b_branch2b/gamma:0' shape=(256,) dtype=float32_ref>,\n",
       " <tf.Variable 'bn4b_branch2b/beta:0' shape=(256,) dtype=float32_ref>,\n",
       " <tf.Variable 'bn4b_branch2c/gamma:0' shape=(1024,) dtype=float32_ref>,\n",
       " <tf.Variable 'bn4b_branch2c/beta:0' shape=(1024,) dtype=float32_ref>,\n",
       " <tf.Variable 'bn4c_branch2a/gamma:0' shape=(256,) dtype=float32_ref>,\n",
       " <tf.Variable 'bn4c_branch2a/beta:0' shape=(256,) dtype=float32_ref>,\n",
       " <tf.Variable 'bn4c_branch2b/gamma:0' shape=(256,) dtype=float32_ref>,\n",
       " <tf.Variable 'bn4c_branch2b/beta:0' shape=(256,) dtype=float32_ref>,\n",
       " <tf.Variable 'bn4c_branch2c/gamma:0' shape=(1024,) dtype=float32_ref>,\n",
       " <tf.Variable 'bn4c_branch2c/beta:0' shape=(1024,) dtype=float32_ref>,\n",
       " <tf.Variable 'bn4d_branch2a/gamma:0' shape=(256,) dtype=float32_ref>,\n",
       " <tf.Variable 'bn4d_branch2a/beta:0' shape=(256,) dtype=float32_ref>,\n",
       " <tf.Variable 'bn4d_branch2b/gamma:0' shape=(256,) dtype=float32_ref>,\n",
       " <tf.Variable 'bn4d_branch2b/beta:0' shape=(256,) dtype=float32_ref>,\n",
       " <tf.Variable 'bn4d_branch2c/gamma:0' shape=(1024,) dtype=float32_ref>,\n",
       " <tf.Variable 'bn4d_branch2c/beta:0' shape=(1024,) dtype=float32_ref>,\n",
       " <tf.Variable 'bn4e_branch2a/gamma:0' shape=(256,) dtype=float32_ref>,\n",
       " <tf.Variable 'bn4e_branch2a/beta:0' shape=(256,) dtype=float32_ref>,\n",
       " <tf.Variable 'bn4e_branch2b/gamma:0' shape=(256,) dtype=float32_ref>,\n",
       " <tf.Variable 'bn4e_branch2b/beta:0' shape=(256,) dtype=float32_ref>,\n",
       " <tf.Variable 'bn4e_branch2c/gamma:0' shape=(1024,) dtype=float32_ref>,\n",
       " <tf.Variable 'bn4e_branch2c/beta:0' shape=(1024,) dtype=float32_ref>,\n",
       " <tf.Variable 'bn4f_branch2a/gamma:0' shape=(256,) dtype=float32_ref>,\n",
       " <tf.Variable 'bn4f_branch2a/beta:0' shape=(256,) dtype=float32_ref>,\n",
       " <tf.Variable 'bn4f_branch2b/gamma:0' shape=(256,) dtype=float32_ref>,\n",
       " <tf.Variable 'bn4f_branch2b/beta:0' shape=(256,) dtype=float32_ref>,\n",
       " <tf.Variable 'bn4f_branch2c/gamma:0' shape=(1024,) dtype=float32_ref>,\n",
       " <tf.Variable 'bn4f_branch2c/beta:0' shape=(1024,) dtype=float32_ref>,\n",
       " <tf.Variable 'bn5a_branch2a/gamma:0' shape=(512,) dtype=float32_ref>,\n",
       " <tf.Variable 'bn5a_branch2a/beta:0' shape=(512,) dtype=float32_ref>,\n",
       " <tf.Variable 'bn5a_branch2b/gamma:0' shape=(512,) dtype=float32_ref>,\n",
       " <tf.Variable 'bn5a_branch2b/beta:0' shape=(512,) dtype=float32_ref>,\n",
       " <tf.Variable 'bn5a_branch2c/gamma:0' shape=(2048,) dtype=float32_ref>,\n",
       " <tf.Variable 'bn5a_branch2c/beta:0' shape=(2048,) dtype=float32_ref>,\n",
       " <tf.Variable 'bn5a_branch1/gamma:0' shape=(2048,) dtype=float32_ref>,\n",
       " <tf.Variable 'bn5a_branch1/beta:0' shape=(2048,) dtype=float32_ref>,\n",
       " <tf.Variable 'bn5b_branch2a/gamma:0' shape=(512,) dtype=float32_ref>,\n",
       " <tf.Variable 'bn5b_branch2a/beta:0' shape=(512,) dtype=float32_ref>,\n",
       " <tf.Variable 'bn5b_branch2b/gamma:0' shape=(512,) dtype=float32_ref>,\n",
       " <tf.Variable 'bn5b_branch2b/beta:0' shape=(512,) dtype=float32_ref>,\n",
       " <tf.Variable 'bn5b_branch2c/gamma:0' shape=(2048,) dtype=float32_ref>,\n",
       " <tf.Variable 'bn5b_branch2c/beta:0' shape=(2048,) dtype=float32_ref>,\n",
       " <tf.Variable 'bn5c_branch2a/gamma:0' shape=(512,) dtype=float32_ref>,\n",
       " <tf.Variable 'bn5c_branch2a/beta:0' shape=(512,) dtype=float32_ref>,\n",
       " <tf.Variable 'bn5c_branch2b/gamma:0' shape=(512,) dtype=float32_ref>,\n",
       " <tf.Variable 'bn5c_branch2b/beta:0' shape=(512,) dtype=float32_ref>,\n",
       " <tf.Variable 'bn5c_branch2c/gamma:0' shape=(2048,) dtype=float32_ref>,\n",
       " <tf.Variable 'bn5c_branch2c/beta:0' shape=(2048,) dtype=float32_ref>,\n",
       " <tf.Variable 'upsample_dense/kernel:0' shape=(2048, 2048) dtype=float32_ref>,\n",
       " <tf.Variable 'upsample_dense/bias:0' shape=(2048,) dtype=float32_ref>,\n",
       " <tf.Variable 'upsample_res5a_branch2a/kernel:0' shape=(1, 1, 2048, 2048) dtype=float32_ref>,\n",
       " <tf.Variable 'upsample_res5a_branch2a/bias:0' shape=(2048,) dtype=float32_ref>,\n",
       " <tf.Variable 'upsample_bn5a_branch2a/gamma:0' shape=(2048,) dtype=float32_ref>,\n",
       " <tf.Variable 'upsample_bn5a_branch2a/beta:0' shape=(2048,) dtype=float32_ref>,\n",
       " <tf.Variable 'upsample_res5a_branch2b/kernel:0' shape=(3, 3, 2048, 512) dtype=float32_ref>,\n",
       " <tf.Variable 'upsample_res5a_branch2b/bias:0' shape=(512,) dtype=float32_ref>,\n",
       " <tf.Variable 'upsample_bn5a_branch2b/gamma:0' shape=(512,) dtype=float32_ref>,\n",
       " <tf.Variable 'upsample_bn5a_branch2b/beta:0' shape=(512,) dtype=float32_ref>,\n",
       " <tf.Variable 'upsample_res5a_branch2c/kernel:0' shape=(1, 1, 512, 512) dtype=float32_ref>,\n",
       " <tf.Variable 'upsample_res5a_branch2c/bias:0' shape=(512,) dtype=float32_ref>,\n",
       " <tf.Variable 'upsample_res5a_branch1/kernel:0' shape=(1, 1, 2048, 512) dtype=float32_ref>,\n",
       " <tf.Variable 'upsample_res5a_branch1/bias:0' shape=(512,) dtype=float32_ref>,\n",
       " <tf.Variable 'upsample_bn5a_branch2c/gamma:0' shape=(512,) dtype=float32_ref>,\n",
       " <tf.Variable 'upsample_bn5a_branch2c/beta:0' shape=(512,) dtype=float32_ref>,\n",
       " <tf.Variable 'upsample_bn5a_branch1/gamma:0' shape=(512,) dtype=float32_ref>,\n",
       " <tf.Variable 'upsample_bn5a_branch1/beta:0' shape=(512,) dtype=float32_ref>,\n",
       " <tf.Variable 'upsample_res5b_branch2a/kernel:0' shape=(1, 1, 512, 2048) dtype=float32_ref>,\n",
       " <tf.Variable 'upsample_res5b_branch2a/bias:0' shape=(2048,) dtype=float32_ref>,\n",
       " <tf.Variable 'upsample_bn5b_branch2a/gamma:0' shape=(2048,) dtype=float32_ref>,\n",
       " <tf.Variable 'upsample_bn5b_branch2a/beta:0' shape=(2048,) dtype=float32_ref>,\n",
       " <tf.Variable 'upsample_res5b_branch2b/kernel:0' shape=(3, 3, 2048, 512) dtype=float32_ref>,\n",
       " <tf.Variable 'upsample_res5b_branch2b/bias:0' shape=(512,) dtype=float32_ref>,\n",
       " <tf.Variable 'upsample_bn5b_branch2b/gamma:0' shape=(512,) dtype=float32_ref>,\n",
       " <tf.Variable 'upsample_bn5b_branch2b/beta:0' shape=(512,) dtype=float32_ref>,\n",
       " <tf.Variable 'upsample_res5b_branch2c/kernel:0' shape=(1, 1, 512, 512) dtype=float32_ref>,\n",
       " <tf.Variable 'upsample_res5b_branch2c/bias:0' shape=(512,) dtype=float32_ref>,\n",
       " <tf.Variable 'upsample_bn5b_branch2c/gamma:0' shape=(512,) dtype=float32_ref>,\n",
       " <tf.Variable 'upsample_bn5b_branch2c/beta:0' shape=(512,) dtype=float32_ref>,\n",
       " <tf.Variable 'upsample_res5c_branch2a/kernel:0' shape=(1, 1, 512, 2048) dtype=float32_ref>,\n",
       " <tf.Variable 'upsample_res5c_branch2a/bias:0' shape=(2048,) dtype=float32_ref>,\n",
       " <tf.Variable 'upsample_bn5c_branch2a/gamma:0' shape=(2048,) dtype=float32_ref>,\n",
       " <tf.Variable 'upsample_bn5c_branch2a/beta:0' shape=(2048,) dtype=float32_ref>,\n",
       " <tf.Variable 'upsample_res5c_branch2b/kernel:0' shape=(3, 3, 2048, 512) dtype=float32_ref>,\n",
       " <tf.Variable 'upsample_res5c_branch2b/bias:0' shape=(512,) dtype=float32_ref>,\n",
       " <tf.Variable 'upsample_bn5c_branch2b/gamma:0' shape=(512,) dtype=float32_ref>,\n",
       " <tf.Variable 'upsample_bn5c_branch2b/beta:0' shape=(512,) dtype=float32_ref>,\n",
       " <tf.Variable 'upsample_res5c_branch2c/kernel:0' shape=(1, 1, 512, 512) dtype=float32_ref>,\n",
       " <tf.Variable 'upsample_res5c_branch2c/bias:0' shape=(512,) dtype=float32_ref>,\n",
       " <tf.Variable 'upsample_bn5c_branch2c/gamma:0' shape=(512,) dtype=float32_ref>,\n",
       " <tf.Variable 'upsample_bn5c_branch2c/beta:0' shape=(512,) dtype=float32_ref>,\n",
       " <tf.Variable 'upsample_res4a_branch2a/kernel:0' shape=(1, 1, 512, 1024) dtype=float32_ref>,\n",
       " <tf.Variable 'upsample_res4a_branch2a/bias:0' shape=(1024,) dtype=float32_ref>,\n",
       " <tf.Variable 'upsample_bn4a_branch2a/gamma:0' shape=(1024,) dtype=float32_ref>,\n",
       " <tf.Variable 'upsample_bn4a_branch2a/beta:0' shape=(1024,) dtype=float32_ref>,\n",
       " <tf.Variable 'upsample_res4a_branch2b/kernel:0' shape=(3, 3, 1024, 256) dtype=float32_ref>,\n",
       " <tf.Variable 'upsample_res4a_branch2b/bias:0' shape=(256,) dtype=float32_ref>,\n",
       " <tf.Variable 'upsample_bn4a_branch2b/gamma:0' shape=(256,) dtype=float32_ref>,\n",
       " <tf.Variable 'upsample_bn4a_branch2b/beta:0' shape=(256,) dtype=float32_ref>,\n",
       " <tf.Variable 'upsample_res4a_branch2c/kernel:0' shape=(1, 1, 256, 256) dtype=float32_ref>,\n",
       " <tf.Variable 'upsample_res4a_branch2c/bias:0' shape=(256,) dtype=float32_ref>,\n",
       " <tf.Variable 'upsample_res4a_branch1/kernel:0' shape=(1, 1, 512, 256) dtype=float32_ref>,\n",
       " <tf.Variable 'upsample_res4a_branch1/bias:0' shape=(256,) dtype=float32_ref>,\n",
       " <tf.Variable 'upsample_bn4a_branch2c/gamma:0' shape=(256,) dtype=float32_ref>,\n",
       " <tf.Variable 'upsample_bn4a_branch2c/beta:0' shape=(256,) dtype=float32_ref>,\n",
       " <tf.Variable 'upsample_bn4a_branch1/gamma:0' shape=(256,) dtype=float32_ref>,\n",
       " <tf.Variable 'upsample_bn4a_branch1/beta:0' shape=(256,) dtype=float32_ref>,\n",
       " <tf.Variable 'upsample_res4b_branch2a/kernel:0' shape=(1, 1, 256, 1024) dtype=float32_ref>,\n",
       " <tf.Variable 'upsample_res4b_branch2a/bias:0' shape=(1024,) dtype=float32_ref>,\n",
       " <tf.Variable 'upsample_bn4b_branch2a/gamma:0' shape=(1024,) dtype=float32_ref>,\n",
       " <tf.Variable 'upsample_bn4b_branch2a/beta:0' shape=(1024,) dtype=float32_ref>,\n",
       " <tf.Variable 'upsample_res4b_branch2b/kernel:0' shape=(3, 3, 1024, 256) dtype=float32_ref>,\n",
       " <tf.Variable 'upsample_res4b_branch2b/bias:0' shape=(256,) dtype=float32_ref>,\n",
       " <tf.Variable 'upsample_bn4b_branch2b/gamma:0' shape=(256,) dtype=float32_ref>,\n",
       " <tf.Variable 'upsample_bn4b_branch2b/beta:0' shape=(256,) dtype=float32_ref>,\n",
       " <tf.Variable 'upsample_res4b_branch2c/kernel:0' shape=(1, 1, 256, 256) dtype=float32_ref>,\n",
       " <tf.Variable 'upsample_res4b_branch2c/bias:0' shape=(256,) dtype=float32_ref>,\n",
       " <tf.Variable 'upsample_bn4b_branch2c/gamma:0' shape=(256,) dtype=float32_ref>,\n",
       " <tf.Variable 'upsample_bn4b_branch2c/beta:0' shape=(256,) dtype=float32_ref>,\n",
       " <tf.Variable 'upsample_res4c_branch2a/kernel:0' shape=(1, 1, 256, 1024) dtype=float32_ref>,\n",
       " <tf.Variable 'upsample_res4c_branch2a/bias:0' shape=(1024,) dtype=float32_ref>,\n",
       " <tf.Variable 'upsample_bn4c_branch2a/gamma:0' shape=(1024,) dtype=float32_ref>,\n",
       " <tf.Variable 'upsample_bn4c_branch2a/beta:0' shape=(1024,) dtype=float32_ref>,\n",
       " <tf.Variable 'upsample_res4c_branch2b/kernel:0' shape=(3, 3, 1024, 256) dtype=float32_ref>,\n",
       " <tf.Variable 'upsample_res4c_branch2b/bias:0' shape=(256,) dtype=float32_ref>,\n",
       " <tf.Variable 'upsample_bn4c_branch2b/gamma:0' shape=(256,) dtype=float32_ref>,\n",
       " <tf.Variable 'upsample_bn4c_branch2b/beta:0' shape=(256,) dtype=float32_ref>,\n",
       " <tf.Variable 'upsample_res4c_branch2c/kernel:0' shape=(1, 1, 256, 256) dtype=float32_ref>,\n",
       " <tf.Variable 'upsample_res4c_branch2c/bias:0' shape=(256,) dtype=float32_ref>,\n",
       " <tf.Variable 'upsample_bn4c_branch2c/gamma:0' shape=(256,) dtype=float32_ref>,\n",
       " <tf.Variable 'upsample_bn4c_branch2c/beta:0' shape=(256,) dtype=float32_ref>,\n",
       " <tf.Variable 'upsample_res4d_branch2a/kernel:0' shape=(1, 1, 256, 1024) dtype=float32_ref>,\n",
       " <tf.Variable 'upsample_res4d_branch2a/bias:0' shape=(1024,) dtype=float32_ref>,\n",
       " <tf.Variable 'upsample_bn4d_branch2a/gamma:0' shape=(1024,) dtype=float32_ref>,\n",
       " <tf.Variable 'upsample_bn4d_branch2a/beta:0' shape=(1024,) dtype=float32_ref>,\n",
       " <tf.Variable 'upsample_res4d_branch2b/kernel:0' shape=(3, 3, 1024, 256) dtype=float32_ref>,\n",
       " <tf.Variable 'upsample_res4d_branch2b/bias:0' shape=(256,) dtype=float32_ref>,\n",
       " <tf.Variable 'upsample_bn4d_branch2b/gamma:0' shape=(256,) dtype=float32_ref>,\n",
       " <tf.Variable 'upsample_bn4d_branch2b/beta:0' shape=(256,) dtype=float32_ref>,\n",
       " <tf.Variable 'upsample_res4d_branch2c/kernel:0' shape=(1, 1, 256, 256) dtype=float32_ref>,\n",
       " <tf.Variable 'upsample_res4d_branch2c/bias:0' shape=(256,) dtype=float32_ref>,\n",
       " <tf.Variable 'upsample_bn4d_branch2c/gamma:0' shape=(256,) dtype=float32_ref>,\n",
       " <tf.Variable 'upsample_bn4d_branch2c/beta:0' shape=(256,) dtype=float32_ref>,\n",
       " <tf.Variable 'upsample_res4e_branch2a/kernel:0' shape=(1, 1, 256, 1024) dtype=float32_ref>,\n",
       " <tf.Variable 'upsample_res4e_branch2a/bias:0' shape=(1024,) dtype=float32_ref>,\n",
       " <tf.Variable 'upsample_bn4e_branch2a/gamma:0' shape=(1024,) dtype=float32_ref>,\n",
       " <tf.Variable 'upsample_bn4e_branch2a/beta:0' shape=(1024,) dtype=float32_ref>,\n",
       " <tf.Variable 'upsample_res4e_branch2b/kernel:0' shape=(3, 3, 1024, 256) dtype=float32_ref>,\n",
       " <tf.Variable 'upsample_res4e_branch2b/bias:0' shape=(256,) dtype=float32_ref>,\n",
       " <tf.Variable 'upsample_bn4e_branch2b/gamma:0' shape=(256,) dtype=float32_ref>,\n",
       " <tf.Variable 'upsample_bn4e_branch2b/beta:0' shape=(256,) dtype=float32_ref>,\n",
       " <tf.Variable 'upsample_res4e_branch2c/kernel:0' shape=(1, 1, 256, 256) dtype=float32_ref>,\n",
       " <tf.Variable 'upsample_res4e_branch2c/bias:0' shape=(256,) dtype=float32_ref>,\n",
       " <tf.Variable 'upsample_bn4e_branch2c/gamma:0' shape=(256,) dtype=float32_ref>,\n",
       " <tf.Variable 'upsample_bn4e_branch2c/beta:0' shape=(256,) dtype=float32_ref>,\n",
       " <tf.Variable 'upsample_res4f_branch2a/kernel:0' shape=(1, 1, 256, 1024) dtype=float32_ref>,\n",
       " <tf.Variable 'upsample_res4f_branch2a/bias:0' shape=(1024,) dtype=float32_ref>,\n",
       " <tf.Variable 'upsample_bn4f_branch2a/gamma:0' shape=(1024,) dtype=float32_ref>,\n",
       " <tf.Variable 'upsample_bn4f_branch2a/beta:0' shape=(1024,) dtype=float32_ref>,\n",
       " <tf.Variable 'upsample_res4f_branch2b/kernel:0' shape=(3, 3, 1024, 256) dtype=float32_ref>,\n",
       " <tf.Variable 'upsample_res4f_branch2b/bias:0' shape=(256,) dtype=float32_ref>,\n",
       " <tf.Variable 'upsample_bn4f_branch2b/gamma:0' shape=(256,) dtype=float32_ref>,\n",
       " <tf.Variable 'upsample_bn4f_branch2b/beta:0' shape=(256,) dtype=float32_ref>,\n",
       " <tf.Variable 'upsample_res4f_branch2c/kernel:0' shape=(1, 1, 256, 256) dtype=float32_ref>,\n",
       " <tf.Variable 'upsample_res4f_branch2c/bias:0' shape=(256,) dtype=float32_ref>,\n",
       " <tf.Variable 'upsample_bn4f_branch2c/gamma:0' shape=(256,) dtype=float32_ref>,\n",
       " <tf.Variable 'upsample_bn4f_branch2c/beta:0' shape=(256,) dtype=float32_ref>,\n",
       " <tf.Variable 'upsample_res3a_branch2a/kernel:0' shape=(1, 1, 256, 512) dtype=float32_ref>,\n",
       " <tf.Variable 'upsample_res3a_branch2a/bias:0' shape=(512,) dtype=float32_ref>,\n",
       " <tf.Variable 'upsample_bn3a_branch2a/gamma:0' shape=(512,) dtype=float32_ref>,\n",
       " <tf.Variable 'upsample_bn3a_branch2a/beta:0' shape=(512,) dtype=float32_ref>,\n",
       " <tf.Variable 'upsample_res3a_branch2b/kernel:0' shape=(3, 3, 512, 128) dtype=float32_ref>,\n",
       " <tf.Variable 'upsample_res3a_branch2b/bias:0' shape=(128,) dtype=float32_ref>,\n",
       " <tf.Variable 'upsample_bn3a_branch2b/gamma:0' shape=(128,) dtype=float32_ref>,\n",
       " <tf.Variable 'upsample_bn3a_branch2b/beta:0' shape=(128,) dtype=float32_ref>,\n",
       " <tf.Variable 'upsample_res3a_branch2c/kernel:0' shape=(1, 1, 128, 128) dtype=float32_ref>,\n",
       " <tf.Variable 'upsample_res3a_branch2c/bias:0' shape=(128,) dtype=float32_ref>,\n",
       " <tf.Variable 'upsample_res3a_branch1/kernel:0' shape=(1, 1, 256, 128) dtype=float32_ref>,\n",
       " <tf.Variable 'upsample_res3a_branch1/bias:0' shape=(128,) dtype=float32_ref>,\n",
       " <tf.Variable 'upsample_bn3a_branch2c/gamma:0' shape=(128,) dtype=float32_ref>,\n",
       " <tf.Variable 'upsample_bn3a_branch2c/beta:0' shape=(128,) dtype=float32_ref>,\n",
       " <tf.Variable 'upsample_bn3a_branch1/gamma:0' shape=(128,) dtype=float32_ref>,\n",
       " <tf.Variable 'upsample_bn3a_branch1/beta:0' shape=(128,) dtype=float32_ref>,\n",
       " <tf.Variable 'upsample_res3b_branch2a/kernel:0' shape=(1, 1, 128, 512) dtype=float32_ref>,\n",
       " <tf.Variable 'upsample_res3b_branch2a/bias:0' shape=(512,) dtype=float32_ref>,\n",
       " <tf.Variable 'upsample_bn3b_branch2a/gamma:0' shape=(512,) dtype=float32_ref>,\n",
       " <tf.Variable 'upsample_bn3b_branch2a/beta:0' shape=(512,) dtype=float32_ref>,\n",
       " <tf.Variable 'upsample_res3b_branch2b/kernel:0' shape=(3, 3, 512, 128) dtype=float32_ref>,\n",
       " <tf.Variable 'upsample_res3b_branch2b/bias:0' shape=(128,) dtype=float32_ref>,\n",
       " <tf.Variable 'upsample_bn3b_branch2b/gamma:0' shape=(128,) dtype=float32_ref>,\n",
       " <tf.Variable 'upsample_bn3b_branch2b/beta:0' shape=(128,) dtype=float32_ref>,\n",
       " <tf.Variable 'upsample_res3b_branch2c/kernel:0' shape=(1, 1, 128, 128) dtype=float32_ref>,\n",
       " <tf.Variable 'upsample_res3b_branch2c/bias:0' shape=(128,) dtype=float32_ref>,\n",
       " <tf.Variable 'upsample_bn3b_branch2c/gamma:0' shape=(128,) dtype=float32_ref>,\n",
       " <tf.Variable 'upsample_bn3b_branch2c/beta:0' shape=(128,) dtype=float32_ref>,\n",
       " <tf.Variable 'upsample_res3c_branch2a/kernel:0' shape=(1, 1, 128, 512) dtype=float32_ref>,\n",
       " <tf.Variable 'upsample_res3c_branch2a/bias:0' shape=(512,) dtype=float32_ref>,\n",
       " <tf.Variable 'upsample_bn3c_branch2a/gamma:0' shape=(512,) dtype=float32_ref>,\n",
       " <tf.Variable 'upsample_bn3c_branch2a/beta:0' shape=(512,) dtype=float32_ref>,\n",
       " <tf.Variable 'upsample_res3c_branch2b/kernel:0' shape=(3, 3, 512, 128) dtype=float32_ref>,\n",
       " <tf.Variable 'upsample_res3c_branch2b/bias:0' shape=(128,) dtype=float32_ref>,\n",
       " <tf.Variable 'upsample_bn3c_branch2b/gamma:0' shape=(128,) dtype=float32_ref>,\n",
       " <tf.Variable 'upsample_bn3c_branch2b/beta:0' shape=(128,) dtype=float32_ref>,\n",
       " <tf.Variable 'upsample_res3c_branch2c/kernel:0' shape=(1, 1, 128, 128) dtype=float32_ref>,\n",
       " <tf.Variable 'upsample_res3c_branch2c/bias:0' shape=(128,) dtype=float32_ref>,\n",
       " <tf.Variable 'upsample_bn3c_branch2c/gamma:0' shape=(128,) dtype=float32_ref>,\n",
       " <tf.Variable 'upsample_bn3c_branch2c/beta:0' shape=(128,) dtype=float32_ref>,\n",
       " <tf.Variable 'upsample_res3d_branch2a/kernel:0' shape=(1, 1, 128, 512) dtype=float32_ref>,\n",
       " <tf.Variable 'upsample_res3d_branch2a/bias:0' shape=(512,) dtype=float32_ref>,\n",
       " <tf.Variable 'upsample_bn3d_branch2a/gamma:0' shape=(512,) dtype=float32_ref>,\n",
       " <tf.Variable 'upsample_bn3d_branch2a/beta:0' shape=(512,) dtype=float32_ref>,\n",
       " <tf.Variable 'upsample_res3d_branch2b/kernel:0' shape=(3, 3, 512, 128) dtype=float32_ref>,\n",
       " <tf.Variable 'upsample_res3d_branch2b/bias:0' shape=(128,) dtype=float32_ref>,\n",
       " <tf.Variable 'upsample_bn3d_branch2b/gamma:0' shape=(128,) dtype=float32_ref>,\n",
       " <tf.Variable 'upsample_bn3d_branch2b/beta:0' shape=(128,) dtype=float32_ref>,\n",
       " <tf.Variable 'upsample_res3d_branch2c/kernel:0' shape=(1, 1, 128, 128) dtype=float32_ref>,\n",
       " <tf.Variable 'upsample_res3d_branch2c/bias:0' shape=(128,) dtype=float32_ref>,\n",
       " <tf.Variable 'upsample_bn3d_branch2c/gamma:0' shape=(128,) dtype=float32_ref>,\n",
       " <tf.Variable 'upsample_bn3d_branch2c/beta:0' shape=(128,) dtype=float32_ref>,\n",
       " <tf.Variable 'upsample_res2a_branch2a/kernel:0' shape=(1, 1, 128, 256) dtype=float32_ref>,\n",
       " <tf.Variable 'upsample_res2a_branch2a/bias:0' shape=(256,) dtype=float32_ref>,\n",
       " <tf.Variable 'upsample_bn2a_branch2a/gamma:0' shape=(256,) dtype=float32_ref>,\n",
       " <tf.Variable 'upsample_bn2a_branch2a/beta:0' shape=(256,) dtype=float32_ref>,\n",
       " <tf.Variable 'upsample_res2a_branch2b/kernel:0' shape=(3, 3, 256, 64) dtype=float32_ref>,\n",
       " <tf.Variable 'upsample_res2a_branch2b/bias:0' shape=(64,) dtype=float32_ref>,\n",
       " <tf.Variable 'upsample_bn2a_branch2b/gamma:0' shape=(64,) dtype=float32_ref>,\n",
       " <tf.Variable 'upsample_bn2a_branch2b/beta:0' shape=(64,) dtype=float32_ref>,\n",
       " <tf.Variable 'upsample_res2a_branch2c/kernel:0' shape=(1, 1, 64, 64) dtype=float32_ref>,\n",
       " <tf.Variable 'upsample_res2a_branch2c/bias:0' shape=(64,) dtype=float32_ref>,\n",
       " <tf.Variable 'upsample_res2a_branch1/kernel:0' shape=(1, 1, 128, 64) dtype=float32_ref>,\n",
       " <tf.Variable 'upsample_res2a_branch1/bias:0' shape=(64,) dtype=float32_ref>,\n",
       " <tf.Variable 'upsample_bn2a_branch2c/gamma:0' shape=(64,) dtype=float32_ref>,\n",
       " <tf.Variable 'upsample_bn2a_branch2c/beta:0' shape=(64,) dtype=float32_ref>,\n",
       " <tf.Variable 'upsample_bn2a_branch1/gamma:0' shape=(64,) dtype=float32_ref>,\n",
       " <tf.Variable 'upsample_bn2a_branch1/beta:0' shape=(64,) dtype=float32_ref>,\n",
       " <tf.Variable 'upsample_res2b_branch2a/kernel:0' shape=(1, 1, 64, 256) dtype=float32_ref>,\n",
       " <tf.Variable 'upsample_res2b_branch2a/bias:0' shape=(256,) dtype=float32_ref>,\n",
       " <tf.Variable 'upsample_bn2b_branch2a/gamma:0' shape=(256,) dtype=float32_ref>,\n",
       " <tf.Variable 'upsample_bn2b_branch2a/beta:0' shape=(256,) dtype=float32_ref>,\n",
       " <tf.Variable 'upsample_res2b_branch2b/kernel:0' shape=(3, 3, 256, 64) dtype=float32_ref>,\n",
       " <tf.Variable 'upsample_res2b_branch2b/bias:0' shape=(64,) dtype=float32_ref>,\n",
       " <tf.Variable 'upsample_bn2b_branch2b/gamma:0' shape=(64,) dtype=float32_ref>,\n",
       " <tf.Variable 'upsample_bn2b_branch2b/beta:0' shape=(64,) dtype=float32_ref>,\n",
       " <tf.Variable 'upsample_res2b_branch2c/kernel:0' shape=(1, 1, 64, 64) dtype=float32_ref>,\n",
       " <tf.Variable 'upsample_res2b_branch2c/bias:0' shape=(64,) dtype=float32_ref>,\n",
       " <tf.Variable 'upsample_bn2b_branch2c/gamma:0' shape=(64,) dtype=float32_ref>,\n",
       " <tf.Variable 'upsample_bn2b_branch2c/beta:0' shape=(64,) dtype=float32_ref>,\n",
       " <tf.Variable 'upsample_res2c_branch2a/kernel:0' shape=(1, 1, 64, 256) dtype=float32_ref>,\n",
       " <tf.Variable 'upsample_res2c_branch2a/bias:0' shape=(256,) dtype=float32_ref>,\n",
       " <tf.Variable 'upsample_bn2c_branch2a/gamma:0' shape=(256,) dtype=float32_ref>,\n",
       " <tf.Variable 'upsample_bn2c_branch2a/beta:0' shape=(256,) dtype=float32_ref>,\n",
       " <tf.Variable 'upsample_res2c_branch2b/kernel:0' shape=(3, 3, 256, 64) dtype=float32_ref>,\n",
       " <tf.Variable 'upsample_res2c_branch2b/bias:0' shape=(64,) dtype=float32_ref>,\n",
       " <tf.Variable 'upsample_bn2c_branch2b/gamma:0' shape=(64,) dtype=float32_ref>,\n",
       " <tf.Variable 'upsample_bn2c_branch2b/beta:0' shape=(64,) dtype=float32_ref>,\n",
       " <tf.Variable 'upsample_res2c_branch2c/kernel:0' shape=(1, 1, 64, 64) dtype=float32_ref>,\n",
       " <tf.Variable 'upsample_res2c_branch2c/bias:0' shape=(64,) dtype=float32_ref>,\n",
       " <tf.Variable 'upsample_bn2c_branch2c/gamma:0' shape=(64,) dtype=float32_ref>,\n",
       " <tf.Variable 'upsample_bn2c_branch2c/beta:0' shape=(64,) dtype=float32_ref>,\n",
       " <tf.Variable 'upsample_conv1/kernel:0' shape=(3, 3, 64, 3) dtype=float32_ref>,\n",
       " <tf.Variable 'upsample_conv1/bias:0' shape=(3,) dtype=float32_ref>,\n",
       " <tf.Variable 'upsample_bn_conv1/gamma:0' shape=(3,) dtype=float32_ref>,\n",
       " <tf.Variable 'upsample_bn_conv1/beta:0' shape=(3,) dtype=float32_ref>]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.trainable_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from skimage.transform import rescale\n",
    "from skimage.io import imread\n",
    "import os\n",
    "import re\n",
    "\n",
    "images_folder = '../patches/'\n",
    "files_list = next(os.walk(images_folder))[2]\n",
    "\n",
    "examples_num = len(files_list) \n",
    "random.seed(17)\n",
    "random.shuffle(files_list)\n",
    "train_set = files_list[:int(examples_num*0.97)]\n",
    "train_num = len(train_set)\n",
    "val_set = files_list[int(examples_num*0.97):]\n",
    "val_num = len(val_set)\n",
    "\n",
    "def data_generator(examples, batch_size=1): #examples is train or val\n",
    "    while 1:\n",
    "        random.shuffle(examples)\n",
    "        images_batch = []\n",
    "        batch_count = 0\n",
    "        for file_name in examples:\n",
    "            batch_count+=1\n",
    "            image = imread(images_folder + file_name)\n",
    "            image = image.astype(np.float32)\n",
    "            image = preprocess_input(image, mode='tf')\n",
    "            images_batch.append(image)\n",
    "            if batch_count==batch_size:\n",
    "                images_batch = np.array(images_batch) #TODO variable size images in batch\n",
    "                yield images_batch, images_batch\n",
    "                images_batch = []\n",
    "                lables_batch = []\n",
    "                batch_count=0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## make logdir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "log_dir = 'log/'\n",
    "if not os.path.isdir(log_dir):\n",
    "    os.mkdir(log_dir)\n",
    "list_log_dir = os.listdir(log_dir)\n",
    "if list_log_dir == []:\n",
    "    log_dir = log_dir + '/0'\n",
    "    os.makedirs(log_dir)\n",
    "else:\n",
    "    log_dir = log_dir+'/{}'.format(np.max([int(i) for i in list_log_dir])+1)\n",
    "    os.makedirs(log_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  start learing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import  TensorBoard\n",
    "from  keras.callbacks import ModelCheckpoint\n",
    "batch_size=1\n",
    "model.fit_generator(generator=data_generator(train_set, batch_size),\\\n",
    "                    epochs=10,steps_per_epoch=train_num/batch_size,\\\n",
    "                    validation_data = data_generator(val_set, batch_size),\\\n",
    "                    validation_steps = val_num/batch_size,\\\n",
    "                    callbacks = [TensorBoard(log_dir = log_dir),\\\n",
    "                                 ModelCheckpoint(filepath = 'learned_model_{val_loss:.2f}.hdf5',\\\n",
    "                                                 save_best_only = True, monitor = 'val_dice_coef',mode='max')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "f, (ax1, ax2) = plt.subplots(1, 2)\n",
    "ax1.imshow(input_image - np.min(input_image))\n",
    "ax2.imshow(res[0]- np.min(res[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.1.5'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
